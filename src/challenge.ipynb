{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu soluci贸n y todas las suposiciones que est谩s considerando. Aqu铆 puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llamada del archivo para su procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"D:\\\\Personal\\\\Prueba_Tecnica\\\\LATAM\\\\farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install py-spy\n",
    "#pip install pyspy\n",
    "#pip install memory-profiler\n",
    "#pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "import cProfile\n",
    "from typing import List, Tuple\n",
    "from datetime import datetime\n",
    "from memory_profiler import memory_usage\n",
    "import emoji\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validaciones iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cara del df \n",
    "\n",
    "data = []\n",
    "# Crear conjunto para la claves 煤nicas \n",
    "unique_keys = set()\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        json_data = json.loads(line.strip())\n",
    "        data.append(json_data)\n",
    "        unique_keys.update(json_data.keys())\n",
    "\n",
    "#Crear el dataframe inicial en funci贸n de los datos le铆dos y con columnas nombradas tal como las claves 煤nicas obtenidas\n",
    "df_1 = pd.DataFrame(data).reindex(columns=unique_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Duplicados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La validaci贸n de duplicados se realiza sobre la columna ID, dado que esta la que contiene los registros 煤nicos de los twits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['id'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vac铆os**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.isnull().sum()/len(df_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respecto a esto se identifca que la columna retweetedTweet tiene el 100% de datos nulos, en su momento si para alg煤n punto se consideran eliminar las columna con alto % de vac铆o se proceder谩, de lo contrario permanecer谩.\n",
    "\n",
    "Por ejemplo, las columans mentioneUsers se podr铆a esperar un alto % de vac铆os dado que en todos los comentarios no necesariamente se etiqueta otra cuenta de \"X\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto 1 datos temporales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Funci贸n inicial**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consideraciones con las que se cre贸 la funci贸n**\n",
    "\n",
    "1. Primero, se carga el archivo JSON utilizando pd.read_json, teniendo en cuenta que est谩 formado por m煤ltiples l铆neas.\n",
    "\n",
    "2. Se agrega una nueva columna al DataFrame para almacenar las fechas en formato date.\n",
    "\n",
    "3. La columna de usuario se normaliza para extraer los datos pertinentes, ya que contiene informaci贸n sobre los usuarios que realizaron las interacciones.\n",
    "\n",
    "4. Para reducir el tama帽o del DataFrame y optimizar el procesamiento, se crea un DataFrame m谩s peque帽o que incluye solo las columnas esenciales, como el ID de la interacci贸n y la fecha, junto con los datos del usuario en columnas separadas.\n",
    "\n",
    "5. Se utiliza groupby para crear una tabla que contenga las fechas, los usuarios y el recuento de interacciones por usuario en cada fecha.\n",
    "\n",
    "6. Se crea una segunda tabla que calcula el total de interacciones por fecha, lo que permite ordenar la tabla por la fecha de manera efectiva.\n",
    "\n",
    "7. Se uner la tabla del punto 5 y 6 y e ordena la tabla resultante de manera que las fechas con mayor actividad aparezcan primero, y dentro de cada fecha, los usuarios m谩s activos se encuentren en la parte superior.\n",
    "\n",
    "8. Se selecciona el primer registro de cada fecha, lo que proporciona el nombre de usuario con la mayor cantidad de interacciones para esa fecha en particular.\n",
    "\n",
    "9. Finalmente, se crea una lista de tuplas que contiene las fechas y los nombres de usuario correspondientes, limitada a los primeros 10 registros para enfocarse en los usuarios m谩s activos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_return_inicial(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    # Leer el archivo JSON y cargarlo en un DataFrame\n",
    "    df = pd.read_json(file_path, lines=True)\n",
    "    \n",
    "    # Modificar la columna de fechas\n",
    "    df['date_tw'] = pd.to_datetime(df['date']).dt.date\n",
    "    \n",
    "    # Normalizar la columna 'user' para obtener datos de usuarios\n",
    "    df_user = json_normalize(df['user'])\n",
    "    \n",
    "    # Unir el DataFrame original con los datos normalizados de usuarios\n",
    "    df = pd.concat([df[['id','date_tw']], df_user.add_prefix('user.')], axis=1)\n",
    "    \n",
    "    # Agrupar por fecha y usuario, contar las interacciones y obtener el total de interacciones por fecha\n",
    "    df_tw_u = df.groupby(['date_tw', 'user.username'])['id'].count().reset_index(name='count_user')\n",
    "    df_tw_ug = df_tw_u.groupby('date_tw')['count_user'].sum().reset_index(name='count_total')\n",
    "    \n",
    "    # Unir los DataFrames y ordenar por conteo total y por usuario\n",
    "    df_tw_u = pd.merge(left=df_tw_u, right=df_tw_ug, how='left', on='date_tw') \\\n",
    "                .sort_values(by=['count_total', 'count_user'], ascending=[False, False])\n",
    "    \n",
    "    # Seleccionar el usuario con m谩s interacciones para cada fecha\n",
    "    df_tw_u_f = df_tw_u.groupby('date_tw').first().reset_index().sort_values(by=['count_total'], ascending=False)\n",
    "    \n",
    "    # Seleccionar las primeras 10 filas y crear una lista de tuplas de fecha y usuario\n",
    "    lista_resultado = list(zip(df_tw_u_f['date_tw'], df_tw_u_f['user.username']))[:10]\n",
    "    \n",
    "    return lista_resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado de ejecuci贸n del a funci贸n**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_return_inicial(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluaci贸n del tiempo de ejecuci贸n**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run('q1_return_inicial(file_path)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memoria total utilizada**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ejecutar la funci贸n memory_usage((q1_return_inicial, (file_path,))), se obtiene la memoria usada durante la ejecuci贸n de la funci贸n q1_return_inicial, as铆 que para obtener el valor total de memoria utilizada se realiza la suma de los valores de la lista. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(memory_usage((q1_return_inicial, (file_path,))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Intentos de optimizaci贸n de la funci贸n inicial**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Funci贸n 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambios:\n",
    "\n",
    "1. En lugar de cargar todo el archivo JSON de una vez, se lee l铆nea por l铆nea para evitar cargar todo el archivo en memoria al mismo tiempo, lo que reduce el uso de memoria.\n",
    "\n",
    "2. En lugar de usar add_prefix, se asignan los nombres de columnas directamente para mayor claridad.\n",
    "\n",
    "3. El filtro para encontrar el usuario con mayor interaccion por fecha no se hace por m茅todo first(), si no que se usa encontrando los 铆ndices de estos usuarios y luego filtrando la tabla que contiene la informaci贸n de fechas nombre de usarios, en esta misma tabla se filtran los 10 registros que pide el ejecicio.\n",
    "\n",
    "4. Se crea un ciclo para la creaci贸n de la lista de datos requeridos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_return_v1(file_path: str) -> List[Tuple[datetime.date, str]] :\n",
    "    # Punto 1: Lista para almacenar los datos JSON\n",
    "    data = []\n",
    "\n",
    "    # Punto 2: Conjunto para almacenar los nombres de las claves 煤nicas\n",
    "    unique_keys = set()\n",
    "\n",
    "    # Punto 3: Abrir el archivo JSON y leer l铆nea por l铆nea\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Punto 4: Cargar la l铆nea como un objeto JSON\n",
    "            json_data = json.loads(line.strip())\n",
    "            \n",
    "            # Punto 5: Agregar el objeto JSON a la lista\n",
    "            data.append(json_data)\n",
    "            \n",
    "            # Punto 6: Actualizar el conjunto de claves 煤nicas\n",
    "            unique_keys.update(json_data.keys())\n",
    "\n",
    "    # Punto 7: Crear DataFrame con los datos JSON\n",
    "    df_inicial = pd.DataFrame(data).reindex(columns=unique_keys)\n",
    "\n",
    "    # Punto 8: Modificar la columna de fechas para obtener el formato AAAA-MM-DD\n",
    "    df_inicial['date_tw'] = pd.to_datetime(df_inicial['date']).dt.date\n",
    "    \n",
    "    # Punto 9: Crear un dataframe con los datos expandidos de los usuarios que luego se une al df inicial\n",
    "    df_user = json_normalize(df_inicial['user'])\n",
    "    ## Se asigna el prefijo a los nombres de las columnas para no perder el origen\n",
    "    df_user.columns = ['user.'+col for col in df_user.columns ]\n",
    "    ## Se une el df de usuarios el df inicial y se eliminan los 铆nidices\n",
    "    df_inicial = pd.concat([df_inicial[['id','date_tw']].copy(),df_user],axis=1).reset_index(drop=True)   \n",
    "    \n",
    "    # Punto 10: Se crean groupby para procesar solo la informaci贸n de fechas y nombres de usuario, agregando la columna del conteo de interacciones de usuarios\n",
    "    df_tw_u = df_inicial.groupby(['date_tw','user.username'])['id'].count().reset_index(name='count_user')\n",
    "    \n",
    "    # Punto 11: Se crea un df nuevo que contiene solo el total de interacciones por fecha.\n",
    "    df_tw_ug = df_tw_u.groupby('date_tw')['count_user'].sum().sort_values(ascending=False).reset_index(name='count_total')\n",
    "    ## Se unen el df original de fechas, usuarios, conteo de interacciones por usuario al df que contiene las fechas e interacciones por fecha, se ordena de tal forma que el conteo total y por usuario es descendente\n",
    "    df_tw_u = pd.merge(left=df_tw_u, right=df_tw_ug, how='left', on='date_tw').sort_values(by=['count_total','count_user'],ascending=[False,False])\n",
    "    ## Se buscan los 铆ndices de la primera fila de cada fecha.\n",
    "    indeices_max = df_tw_u.groupby('date_tw')['count_user'].idxmax()\n",
    "    ## Se filtra del df de twits de tal forma que se ordene de la fecha de mayor interaccion a la menor, solo los primeros 10 registros\n",
    "    df_tw_u_f = df_tw_u.loc[indeices_max].sort_values(by=['count_total'], ascending=False).iloc[0:10,::].reset_index(drop=True)\n",
    "    \n",
    "    # Punto 12: Se crea un ciclo para llenar una lista con la tuplas de fecha y el usuario con mayor cantidad de interacciones en X para ese d铆a \n",
    "    lista_resultado = []\n",
    "    for i in range(len(df_tw_u_f)):\n",
    "        lista_resultado.append((df_tw_u_f['date_tw'][i],df_tw_u_f['user.username'][i]))\n",
    "    \n",
    "    return lista_resultado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado de ejecuci贸n del a funci贸n**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_return_v1(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluaci贸n del tiempo de ejecuci贸n**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run('q1_return_v1(file_path)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memoria total utilizada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(memory_usage((q1_return_v1, (file_path,))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Funci贸n 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambios:\n",
    "\n",
    "Si bien se mantiene una estructura muy similar a la de la funci贸n 1, se realizan algunos peque帽os cambios:\n",
    "\n",
    "1. Al crear el concat entre el DataFrame inicila y el DataFrame de usuarios, en el segundo caso se usa solo la columna objetivo, que es el nombre de usuario.\n",
    "2. La lista de resultados se realiza con una lista de compresi贸n en iteraci贸n sobre las l铆neas del df final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_return_t2(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    data = []\n",
    "    unique_keys = set()\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            json_data = json.loads(line.strip())\n",
    "            data.append(json_data)\n",
    "            unique_keys.update(json_data.keys())\n",
    "\n",
    "    df_inicial = pd.DataFrame(data).reindex(columns=unique_keys)\n",
    "    df_inicial['date_tw'] = pd.to_datetime(df_inicial['date']).dt.date\n",
    "\n",
    "    df_user = json_normalize(df_inicial['user'])\n",
    "    df_user.columns = ['user.'+col for col in df_user.columns]\n",
    "    df_inicial = pd.concat([df_inicial[['id','date_tw']], df_user['user.username']], axis=1).reset_index(drop=True)\n",
    "\n",
    "    df_tw_u = df_inicial.groupby(['date_tw', 'user.username'])['id'].count().reset_index(name='count_user')\n",
    "    df_tw_ug = df_tw_u.groupby('date_tw')['count_user'].sum().reset_index(name='count_total')\n",
    "\n",
    "    df_tw_u = pd.merge(left=df_tw_u, right=df_tw_ug, how='left', on='date_tw').sort_values(by=['count_total', 'count_user'], ascending=[False, False])\n",
    "\n",
    "    indices_max = df_tw_u.groupby('date_tw')['count_user'].idxmax()\n",
    "    df_tw_u_f = df_tw_u.loc[indices_max].sort_values(by=['count_total'], ascending=False).iloc[0:10].reset_index(drop=True)\n",
    "\n",
    "    lista_resultado = [(row['date_tw'], row['user.username']) for _, row in df_tw_u_f.iterrows()]\n",
    "\n",
    "    return lista_resultado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado de ejecuci贸n del a funci贸n**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_return_t2(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluaci贸n del tiempo de ejecuci贸n**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run('q1_return_t2(file_path)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memoria total utilizada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(memory_usage((q1_return_t2, (file_path,))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto 2 emojies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Funci贸n inicial**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consideraciones con las que se cre贸 la funci贸n**\n",
    "\n",
    "1. Primero, se carga el archivo JSON utilizando pd.read_json, teniendo en cuenta que est谩 formado por m煤ltiples l铆neas. La forma de carga adoptada tiene el sentido de no sobrecargar la memoria con una carga completa.\n",
    "\n",
    "2. Las claves 煤nicas tiene el sentido de asegurar que al conformar el dataframe luego de leer los datos van a estar encabezados de forma adecuada.\n",
    "\n",
    "3. Se realiza el uso de la librer铆a emoji y dentro de ella el atriburo EMOJI_DATA, el cual contiene la informaci贸n asociada a los emojies.\n",
    "\n",
    "4. Se crea una funci贸n para extraer los emojies de cada fila de contenido, de esta manera crea una lista de emojies para cada fila de content.\n",
    "\n",
    "5. Se crea una columna dentro del dataFrame para poder almancenas los emojies encontrados en cada fila del content.\n",
    "\n",
    "6. Se crea una lista de vac铆a de emojies a la cual se va integrando con las listas de los emojies encontrados en cada fila de la colimna 'emojis'\n",
    "\n",
    "7. Se crea un dataframe con la lista de los emojies, luego se crea un nuevo df sobre este luego de hacer el conteo descente de aparici贸n para cada emojie.\n",
    "\n",
    "8. Se filtran los datos de color de los emojies, dado que por el momento, el enfoque se centra en identificar la forma b谩sica de los emojis y no en considerar sus modificadores de color. Esto puede ayudar a evitar confusiones sobre el alcance y los objetivos del an谩lisis actual.\n",
    "\n",
    "9. Finalmente, se crea una lista de tuplas que contiene los emojies y la cantidad de apariciones, limitada a los primeros 10 registros para enfocarse en los usuarios m谩s activos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_run_inicial(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Leer el archivo JSON y cargarlo en un DataFrame\n",
    "    data = []\n",
    "    # Crear conjunto para la claves 煤nicas \n",
    "    unique_keys = set()\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            json_data = json.loads(line.strip())\n",
    "            data.append(json_data)\n",
    "            unique_keys.update(json_data.keys())\n",
    "\n",
    "    #Crear el dataframe inicial en funci贸n de los datos le铆dos y con columnas nombradas tal como las claves 煤nicas obtenidas\n",
    "    df_inicial = pd.DataFrame(data).reindex(columns=unique_keys)\n",
    "    \n",
    "    #Usando la librer铆a de emoji se crea una lista con los emojies \"c\" dentro un texto que se pasa, a la funci贸n\n",
    "    def extraer_emojis(texto):\n",
    "        return [c for c in texto if c in emoji.EMOJI_DATA]\n",
    "\n",
    "    #Se aplica la funci贸n sobre la columna de contenido y se crea una nueva con el fin de procesar los emojies posteriormente\n",
    "    df_inicial['emojis'] = df_inicial['content'].apply(extraer_emojis)\n",
    "    \n",
    "    lista_emojies = []\n",
    "\n",
    "    #Se crea un ciclo para obtener todos los emojies en una sola lista.\n",
    "    for fila in df_inicial['emojis']:\n",
    "        lista_emojies.extend(fila)\n",
    "        \n",
    "    df_emojies = pd.DataFrame(lista_emojies,columns=['emojie'])\n",
    "    \n",
    "    df_ef = pd.DataFrame(df_emojies.value_counts(),columns=['Cuenta']).reset_index()\n",
    "    \n",
    "    #Se filtran los s铆mbolos que representan la modificaci贸n de color de los emojies, se crea esta funci贸n bajo el supuesto que por ahora solo se necesita la figura del emojie no su color.\n",
    "    df_ef = df_ef[~df_ef['emojie'].isin(['','','','',''])][0:10]\n",
    "    \n",
    "    lista_resultado = [(row['emojie'], row['Cuenta']) for _, row in df_ef.iterrows()]\n",
    "\n",
    "    return lista_resultado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado de ejecuci贸n del a funci贸n**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_run_inicial(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluaci贸n del tiempo de ejecuci贸n**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run('q2_run_inicial(file_path)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memoria total utilizada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(memory_usage((q2_run_inicial, (file_path,))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Intentos de optimizaci贸n de la funci贸n inicial**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Funci贸n 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_run_v1(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Leer el archivo JSON y cargarlo en un DataFrame\n",
    "    data = []\n",
    "    # Crear conjunto para la claves 煤nicas \n",
    "    unique_keys = set()\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            json_data = json.loads(line.strip())\n",
    "            data.append(json_data)\n",
    "            unique_keys.update(json_data.keys())\n",
    "\n",
    "    #Crear el dataframe inicial en funci贸n de los datos le铆dos y con columnas nombradas tal como las claves 煤nicas obtenidas\n",
    "    df_inicial = pd.DataFrame(data).reindex(columns=unique_keys)\n",
    "    \n",
    "    #Usando la librer铆a de emoji se crea una lista con los emojies \"c\" dentro un texto que se pasa, a la funci贸n\n",
    "    def extraer_emojis(texto):\n",
    "        return [c for c in texto if c in emoji.EMOJI_DATA]\n",
    "\n",
    "    #Se aplica la funci贸n sobre la columna de contenido y se crea una nueva con el fin de procesar los emojies posteriormente\n",
    "    df_inicial['emojis'] = df_inicial['content'].apply(extraer_emojis)\n",
    "    \n",
    "    lista_emojies = []\n",
    "\n",
    "    #Se crea un ciclo para obtener todos los emojies en una sola lista.\n",
    "    for fila in df_inicial['emojis']:\n",
    "        lista_emojies.extend(fila)\n",
    "\n",
    "    df_ef = pd.DataFrame(pd.DataFrame(lista_emojies,columns=['emojie']).value_counts(),columns=['Cuenta']).reset_index()\n",
    "    \n",
    "    #Se filtran los s铆mbolos que representan la modificaci贸n de color de los emojies, se crea esta funci贸n bajo el supuesto que por ahora solo se necesita la figura del emojie no su color.\n",
    "    df_ef = df_ef[~df_ef['emojie'].isin(['','','','',''])][0:10]\n",
    "    \n",
    "    lista_resultado = [(row['emojie'], row['Cuenta']) for _, row in df_ef.iterrows()]\n",
    "\n",
    "    return lista_resultado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado de ejecuci贸n del a funci贸n**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_run_v1(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluaci贸n del tiempo de ejecuci贸n**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run('q2_run_v1(file_path)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memoria total utilizada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(memory_usage((q2_run_v1, (file_path,))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_run_v2(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Leer el archivo JSON y cargarlo en un DataFrame\n",
    "    data = []\n",
    "    # Crear conjunto para la claves 煤nicas \n",
    "    unique_keys = set()\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            json_data = json.loads(line.strip())\n",
    "            data.append(json_data)\n",
    "            unique_keys.update(json_data.keys())\n",
    "\n",
    "    #Crear el dataframe inicial en funci贸n de los datos le铆dos y con columnas nombradas tal como las claves 煤nicas obtenidas\n",
    "    df_inicial = pd.DataFrame(data).reindex(columns=unique_keys)\n",
    "    \n",
    "    #Usando la librer铆a de emoji se crea una lista con los emojies \"c\" dentro un texto que se pasa, a la funci贸n\n",
    "    def extraer_emojis(texto):\n",
    "        return [c for c in texto if c in emoji.EMOJI_DATA]\n",
    "\n",
    "    #Se aplica la funci贸n sobre la columna de contenido y se crea una nueva con el fin de procesar los emojies posteriormente\n",
    "    df_inicial['emojis'] = df_inicial['content'].apply(extraer_emojis)\n",
    "    \n",
    "    # Crear una lista plana con todos los emojis encontrados en todas las filas\n",
    "    lista_emojies = [emoji for fila in df_inicial['emojis'] for emoji in fila]\n",
    "    \n",
    "    # Crear DataFrame con los emojis y su frecuencia\n",
    "    df_ef = pd.DataFrame(Counter(lista_emojies).most_common(), columns=['emojie', 'Cuenta'])\n",
    "        \n",
    "    #Se filtran los s铆mbolos que representan la modificaci贸n de color de los emojies, se crea esta funci贸n bajo el supuesto que por ahora solo se necesita la figura del emojie no su color.\n",
    "    df_ef = df_ef[~df_ef['emojie'].isin(['','','','',''])][0:10]\n",
    "    \n",
    "    lista_resultado = [(row['emojie'], row['Cuenta']) for _, row in df_ef.iterrows()]\n",
    "\n",
    "    return lista_resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado de ejecuci贸n del a funci贸n**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_run_v2(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluaci贸n del tiempo de ejecuci贸n**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run('q2_run_v2(file_path)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memoria total utilizada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(memory_usage((q2_run_v2, (file_path,))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto 3 influyentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **An谩lisis de menci贸n a los usuarios**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se evidencia que tanto en la columna content como en la columna mentionedUsers hat menci贸n a los usuarios, por tanto, es necesario realizar la exploraci贸n aleatoria de si los usuarios mencionados en content con los mismo de mentionedUsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qinicial(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Leer el archivo JSON y cargarlo en un DataFrame\n",
    "    data = []\n",
    "    # Crear conjunto para la claves 煤nicas \n",
    "    unique_keys = set()\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            json_data = json.loads(line.strip())\n",
    "            data.append(json_data)\n",
    "            unique_keys.update(json_data.keys())\n",
    "\n",
    "    #Crear el dataframe inicial en funci贸n de los datos le铆dos y con columnas nombradas tal como las claves 煤nicas obtenidas\n",
    "    df_inicial = pd.DataFrame(data).reindex(columns=unique_keys)\n",
    "\n",
    "    return df_inicial\n",
    "\n",
    "resultado2 = qinicial(file_path)[['content','mentionedUsers']].copy().explode(column = 'mentionedUsers')\n",
    "resultado2= resultado2[~resultado2['mentionedUsers'].isnull()].reset_index()\n",
    "pd.set_option('display.max_colwidth', 500) \n",
    "\n",
    "display(resultado2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al observar el caso anterior se deja en evidencia casos como los de los registros de las fila 117388, el cual tiene indicados dos veces a la misma persona, en este caso se deber铆a poder entender la necesidad del cliente final, dado que una peersona comentada dos veces en un mismo twit puede indicar que la persona es relevante para quien actual en \"X\", pero al tiempo podr铆a mostrar un dato desviado, dado que si solo queremos conocer a quien se han etiquetado por comentario, pues no importa su n煤mero de veces etiquetado, se deber谩 contar una 煤nica vez.\n",
    "\n",
    "Por el momento, aludiendo a que si hay una etiqueta m煤ltiple para una misma cuenta en un solo comentario podr铆a indicar un sentido de relevancia para quien redacta el mensaje, se trasladar谩 esa urgencia al an谩lisis de los m谩s etiquetados, de tal forma que se obseve en la lista la urgencia o importancia que le dan los usuario de \"X\" a quienes etiquetan. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Funci贸n inicial**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado la explicaci贸n anterior se va a hacer uso de la columna mentionedUsers para obtener la informaci贸n de la mensi贸n de usuarios dentro del contenido de una publicaci贸n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_run_inicial(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Leer el archivo JSON y cargarlo en un DataFrame\n",
    "    data = []\n",
    "    # Crear conjunto para la claves 煤nicas \n",
    "    unique_keys = set()\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            json_data = json.loads(line.strip())\n",
    "            data.append(json_data)\n",
    "            unique_keys.update(json_data.keys())\n",
    "\n",
    "    #Crear el dataframe inicial en funci贸n de los datos le铆dos y con columnas nombradas tal como las claves 煤nicas obtenidas\n",
    "    df_inicial = pd.DataFrame(data).reindex(columns=unique_keys)\n",
    "    \n",
    "    #Dado que se ha validado previamente que no hay twits duplicados, se puede usar la columna mentionedUsers, para ser explotada (explode) y luego expandida (json_normalize) y con esta hacer todo el procesamiento solicitado.\n",
    "    \n",
    "    df_relevantes =  json_normalize(pd.DataFrame(df_inicial['mentionedUsers']).dropna().explode(column='mentionedUsers').reset_index(drop=True)['mentionedUsers'])\n",
    "    \n",
    "    # Crear DataFrame con los emojis y su frecuencia\n",
    "    df_relev = pd.DataFrame(Counter(df_relevantes['username']).most_common(), columns=['username', 'Cuenta'])\n",
    "    \n",
    "    lista_resultado = [(row['username'], row['Cuenta']) for _, row in df_relev.iterrows()][:10]\n",
    "    \n",
    "    return lista_resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado de ejecuci贸n del a funci贸n**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 2265),\n",
       " ('Kisanektamorcha', 1840),\n",
       " ('RakeshTikaitBKU', 1644),\n",
       " ('PMOIndia', 1427),\n",
       " ('RahulGandhi', 1146),\n",
       " ('GretaThunberg', 1048),\n",
       " ('RaviSinghKA', 1019),\n",
       " ('rihanna', 986),\n",
       " ('UNHumanRights', 962),\n",
       " ('meenaharris', 926)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_run_inicial(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluaci贸n del tiempo de ejecuci贸n**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         9856895 function calls (9722860 primitive calls) in 16.618 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    1.107    1.107   15.906   15.906 1861183408.py:1(q3_run_inicial)\n",
      "        1    0.021    0.021    1.002    1.002 1861183408.py:23(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(all)\n",
      "        3    0.000    0.000    0.001    0.000 <__array_function__ internals>:177(argsort)\n",
      "        3    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(atleast_2d)\n",
      "        7    0.000    0.000    0.042    0.006 <__array_function__ internals>:177(concatenate)\n",
      "       59    0.000    0.000    0.003    0.000 <__array_function__ internals>:177(copyto)\n",
      "        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(delete)\n",
      "        1    0.000    0.000    0.001    0.001 <__array_function__ internals>:177(prod)\n",
      "        3    0.000    0.000    0.042    0.014 <__array_function__ internals>:177(vstack)\n",
      "       16    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:1053(_handle_fromlist)\n",
      "        1    0.709    0.709   16.615   16.615 <string>:1(<module>)\n",
      "   117407    0.116    0.000   10.386    0.000 __init__.py:299(loads)\n",
      "        1    0.000    0.000    0.035    0.035 __init__.py:565(__init__)\n",
      "        1    0.000    0.000    0.003    0.003 __init__.py:588(most_common)\n",
      "        1    0.000    0.000    0.035    0.035 __init__.py:640(update)\n",
      "       12    0.000    0.000    0.000    0.000 _asarray.py:109(<setcomp>)\n",
      "       12    0.001    0.000    0.001    0.000 _asarray.py:22(require)\n",
      "        1    0.000    0.000    0.000    0.000 _collections_abc.py:315(__subclasshook__)\n",
      "        5    0.000    0.000    0.000    0.000 _decorators.py:214(_format_argument_list)\n",
      "        5    0.000    0.000    0.025    0.005 _decorators.py:302(wrapper)\n",
      "        2    0.000    0.000    0.056    0.028 _decorators.py:322(wrapper)\n",
      "       16    0.000    0.000    0.000    0.000 _dtype.py:24(_kind_name)\n",
      "       16    0.000    0.000    0.000    0.000 _dtype.py:330(_name_includes_bit_suffix)\n",
      "       16    0.000    0.000    0.001    0.000 _dtype.py:344(_name_get)\n",
      "        2    0.001    0.000    0.003    0.001 _methods.py:38(_amax)\n",
      "        1    0.000    0.000    0.000    0.000 _methods.py:46(_sum)\n",
      "        3    0.000    0.000    0.000    0.000 _methods.py:54(_any)\n",
      "        2    0.000    0.000    0.000    0.000 _methods.py:60(_all)\n",
      "   103403    0.040    0.000    0.052    0.000 _normalize.py:122(_normalise_json)\n",
      "   103403    0.201    0.000    1.218    0.000 _normalize.py:163(_normalise_json_ordered)\n",
      "   103403    0.436    0.000    0.580    0.000 _normalize.py:178(<dictcomp>)\n",
      "   103403    0.219    0.000    0.359    0.000 _normalize.py:180(<dictcomp>)\n",
      " 103404/1    0.053    0.000    1.323    1.323 _normalize.py:188(_simple_json_normalize)\n",
      "        1    0.047    0.047    1.323    1.323 _normalize.py:236(<listcomp>)\n",
      "        1    0.076    0.076    1.900    1.900 _normalize.py:241(_json_normalize)\n",
      "        2    0.000    0.000    0.000    0.000 _ufunc_config.py:131(geterr)\n",
      "        2    0.000    0.000    0.000    0.000 _ufunc_config.py:32(seterr)\n",
      "        1    0.000    0.000    0.000    0.000 _ufunc_config.py:425(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 _ufunc_config.py:429(__enter__)\n",
      "        1    0.000    0.000    0.000    0.000 _ufunc_config.py:434(__exit__)\n",
      "        2    0.000    0.000    0.000    0.000 _validators.py:122(_check_for_invalid_keys)\n",
      "        2    0.000    0.000    0.000    0.000 _validators.py:135(validate_kwargs)\n",
      "        1    0.000    0.000    0.000    0.000 _validators.py:162(validate_args_and_kwargs)\n",
      "        6    0.000    0.000    0.000    0.000 _validators.py:218(validate_bool_kwarg)\n",
      "        1    0.000    0.000    0.000    0.000 _validators.py:23(_check_arg_length)\n",
      "        2    0.000    0.000    0.000    0.000 _validators.py:258(validate_axis_style_args)\n",
      "        2    0.000    0.000    0.000    0.000 _validators.py:43(_check_for_default_values)\n",
      "       21    0.000    0.000    0.000    0.000 abc.py:117(__instancecheck__)\n",
      "        1    0.000    0.000    0.000    0.000 abc.py:121(__subclasscheck__)\n",
      "        2    0.000    0.000    0.001    0.000 algorithms.py:1356(take)\n",
      "        2    0.000    0.000    0.000    0.000 api.py:177(union_indexes)\n",
      "        5    0.000    0.000    0.000    0.000 api.py:331(default_index)\n",
      "        2    0.000    0.000    0.001    0.000 base.py:1106(take)\n",
      "        2    0.000    0.000    0.000    0.000 base.py:1132(_maybe_disallow_fill)\n",
      "        1    0.000    0.000    0.001    0.001 base.py:1194(repeat)\n",
      "        1    0.000    0.000    0.013    0.013 base.py:143(isna)\n",
      "       21    0.000    0.000    0.000    0.000 base.py:1658(name)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:1712(_set_names)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:1741(set_names)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:1852(rename)\n",
      "        3    0.000    0.000    0.000    0.000 base.py:1906(nlevels)\n",
      "        1    0.000    0.000    0.010    0.010 base.py:207(join)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:210(interleaved_dtype)\n",
      "        2    0.000    0.000    0.002    0.001 base.py:2162(is_monotonic)\n",
      "        1    0.002    0.002    0.002    0.002 base.py:2170(is_monotonic_increasing)\n",
      "        6    0.008    0.001    0.010    0.002 base.py:2240(is_unique)\n",
      "        4    0.000    0.000    0.000    0.000 base.py:2280(is_boolean)\n",
      "       22    0.000    0.000    0.000    0.000 base.py:229(construct_from_string)\n",
      "        9    0.000    0.000    0.000    0.000 base.py:229(disallow_kwargs)\n",
      "    30482    0.008    0.000    0.009    0.000 base.py:2352(is_floating)\n",
      "        8    0.000    0.000    0.000    0.000 base.py:2604(inferred_type)\n",
      "        2    0.000    0.000    0.001    0.001 base.py:2611(_is_all_dates)\n",
      "       11    0.000    0.000    0.000    0.000 base.py:2632(_is_multi)\n",
      "        2    0.000    0.000    0.000    0.000 base.py:2650(_na_value)\n",
      "       17    0.000    0.000    0.000    0.000 base.py:286(is_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:3058(_validate_sort_keyword)\n",
      "       14    0.000    0.000    0.000    0.000 base.py:326(ndim)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:3282(intersection)\n",
      "        1    0.004    0.004    0.004    0.004 base.py:329(_left_indexer)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:3562(_assert_can_do_setop)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:3568(_convert_can_do_setop)\n",
      "    30482    0.023    0.000    0.072    0.000 base.py:3585(get_loc)\n",
      "        3    0.000    0.000    0.002    0.001 base.py:3714(get_indexer)\n",
      "        1    0.000    0.000    0.001    0.001 base.py:3794(_get_indexer)\n",
      "        3    0.000    0.000    0.000    0.000 base.py:3837(_check_indexing_method)\n",
      "     16/9    0.000    0.000    0.018    0.002 base.py:397(__new__)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:4106(_validate_can_reindex)\n",
      "        1    0.000    0.000    0.009    0.009 base.py:4123(reindex)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:4240(_wrap_reindex_result)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:4244(_maybe_preserve_names)\n",
      "        1    0.000    0.000    0.010    0.010 base.py:4328(join)\n",
      "        5    0.000    0.000    0.000    0.000 base.py:45(__len__)\n",
      "        3    0.000    0.000    0.000    0.000 base.py:465(find)\n",
      "        1    0.000    0.000    0.004    0.004 base.py:4730(_join_monotonic)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:4776(_wrap_joined_index)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:4788(_can_use_libjoin)\n",
      "       85    0.000    0.000    0.000    0.000 base.py:4834(_values)\n",
      "       10    0.000    0.000    0.000    0.000 base.py:4860(_get_engine_target)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:4871(_from_join_target)\n",
      "        4    0.000    0.000    0.000    0.000 base.py:4987(__contains__)\n",
      "       10    0.001    0.000    0.001    0.000 base.py:5037(__getitem__)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:5106(append)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:5131(<setcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:5136(_concat)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:5140(<listcomp>)\n",
      "        8    0.000    0.000    0.000    0.000 base.py:518(<genexpr>)\n",
      "        8    0.000    0.000    0.001    0.000 base.py:5192(equals)\n",
      "        2    0.000    0.000    0.000    0.000 base.py:5279(identical)\n",
      "        2    0.000    0.000    0.000    0.000 base.py:5292(<genexpr>)\n",
      "       16    0.000    0.000    0.000    0.000 base.py:53(shape)\n",
      "        9    0.000    0.000    0.000    0.000 base.py:540(_ensure_array)\n",
      "       48    0.000    0.000    0.000    0.000 base.py:55(<genexpr>)\n",
      "        9    0.000    0.000    0.000    0.000 base.py:554(_dtype_to_subclass)\n",
      "    30478    0.020    0.000    0.055    0.000 base.py:5660(_get_values_for_loc)\n",
      "        3    0.000    0.000    0.000    0.000 base.py:57(_validate_set_axis)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:5758(get_indexer_for)\n",
      "        6    0.000    0.000    0.006    0.001 base.py:5919(_index_as_unique)\n",
      "        3    0.000    0.000    0.000    0.000 base.py:5931(_maybe_promote)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:5977(_find_common_type_compat)\n",
      "        2    0.000    0.000    0.000    0.000 base.py:6018(_should_compare)\n",
      "        2    0.000    0.000    0.000    0.000 base.py:6036(_is_comparable_dtype)\n",
      "    30482    0.017    0.000    0.038    0.000 base.py:6298(_maybe_cast_indexer)\n",
      "        3    0.000    0.000    0.000    0.000 base.py:6307(_maybe_cast_listlike_indexer)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:6535(delete)\n",
      "       20    0.000    0.000    0.000    0.000 base.py:654(_simple_new)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:6632(drop)\n",
      "        7    0.000    0.000    0.020    0.003 base.py:672(_with_infer)\n",
      "        2    0.000    0.000    0.000    0.000 base.py:692(_constructor)\n",
      "    15269    0.004    0.000    0.028    0.000 base.py:7004(ensure_index)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:7063(ensure_has_len)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:7094(_validate_join_method)\n",
      "    15259    0.006    0.000    0.012    0.000 base.py:7099(maybe_extract_name)\n",
      "        9    0.001    0.000    0.006    0.001 base.py:7123(_maybe_cast_data_without_dtype)\n",
      "        2    0.000    0.000    0.000    0.000 base.py:7185(unpack_nested_dtype)\n",
      "        5    0.000    0.000    0.000    0.000 base.py:744(__iter__)\n",
      "        5    0.000    0.000    0.000    0.000 base.py:785(_view)\n",
      "        8    0.000    0.000    0.000    0.000 base.py:803(is_)\n",
      "       25    0.000    0.000    0.000    0.000 base.py:834(_reset_identity)\n",
      "        7    0.001    0.000    0.001    0.000 base.py:845(_engine)\n",
      "    30537    0.007    0.000    0.010    0.000 base.py:884(__len__)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:890(__array__)\n",
      "       12    0.000    0.000    0.000    0.000 base.py:945(dtype)\n",
      "        4    0.000    0.000    0.000    0.000 base.py:982(view)\n",
      "        3    0.000    0.000    0.042    0.014 blocks.py:1116(take_nd)\n",
      "        8    0.001    0.000    0.002    0.000 blocks.py:166(_consolidate_key)\n",
      "        1    0.000    0.000    0.000    0.000 blocks.py:178(_can_hold_na)\n",
      "    15252    0.015    0.000    0.027    0.000 blocks.py:1962(maybe_coerce_values)\n",
      "    15263    0.016    0.000    0.021    0.000 blocks.py:1991(get_block_type)\n",
      "        1    0.000    0.000    0.000    0.000 blocks.py:201(is_bool)\n",
      "        5    0.000    0.000    0.000    0.000 blocks.py:2032(new_block_2d)\n",
      "    15246    0.028    0.000    0.101    0.000 blocks.py:2043(new_block)\n",
      "    15246    0.014    0.000    0.022    0.000 blocks.py:2057(check_ndim)\n",
      "        9    0.000    0.000    0.000    0.000 blocks.py:2121(extend_blocks)\n",
      "        2    0.000    0.000    0.000    0.000 blocks.py:222(get_values)\n",
      "        3    0.000    0.000    0.000    0.000 blocks.py:238(fill_value)\n",
      "       11    0.000    0.000    0.000    0.000 blocks.py:244(mgr_locs)\n",
      "        4    0.000    0.000    0.000    0.000 blocks.py:252(make_block)\n",
      "        3    0.000    0.000    0.000    0.000 blocks.py:267(make_block_same_class)\n",
      "        3    0.000    0.000    0.000    0.000 blocks.py:350(shape)\n",
      "       19    0.000    0.000    0.000    0.000 blocks.py:354(dtype)\n",
      "        5    0.000    0.000    0.000    0.000 blocks.py:358(iget)\n",
      "        2    0.000    0.000    0.013    0.007 blocks.py:396(apply)\n",
      "        1    0.000    0.000    0.000    0.000 blocks.py:406(reduce)\n",
      "        2    0.000    0.000    0.000    0.000 blocks.py:427(_split_op_result)\n",
      "        1    0.000    0.000    0.000    0.000 blocks.py:560(astype)\n",
      "        2    0.000    0.000    0.002    0.001 blocks.py:638(copy)\n",
      "        1    0.000    0.000    0.000    0.000 cast.py:1199(astype_array)\n",
      "        1    0.000    0.000    0.000    0.000 cast.py:1246(astype_array_safe)\n",
      "    15312    0.033    0.000    0.126    0.000 cast.py:1466(maybe_infer_to_datetimelike)\n",
      "       44    0.001    0.000    0.038    0.001 cast.py:1579(maybe_cast_to_datetime)\n",
      "        8    0.000    0.000    0.000    0.000 cast.py:1711(sanitize_to_nanoseconds)\n",
      "        2    0.000    0.000    0.000    0.000 cast.py:1789(find_common_type)\n",
      "        3    0.000    0.000    0.000    0.000 cast.py:1819(<genexpr>)\n",
      "        2    0.000    0.000    0.000    0.000 cast.py:1828(<genexpr>)\n",
      "        2    0.000    0.000    0.000    0.000 cast.py:1830(<genexpr>)\n",
      "        3    0.000    0.000    0.000    0.000 cast.py:1835(<genexpr>)\n",
      "        7    0.002    0.000    0.006    0.001 cast.py:1962(construct_1d_object_array_from_listlike)\n",
      "        6    0.001    0.000    0.001    0.000 cast.py:468(maybe_promote)\n",
      "        1    0.000    0.000    0.000    0.000 cast.py:674(infer_dtype_from)\n",
      "        1    0.000    0.000    0.000    0.000 cast.py:798(infer_dtype_from_array)\n",
      "        1    0.000    0.000    0.000    0.000 codecs.py:260(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 codecs.py:309(__init__)\n",
      "    49772    0.041    0.000    0.090    0.000 codecs.py:319(decode)\n",
      "        2    0.000    0.000    0.000    0.000 common.py:105(is_bool_indexer)\n",
      "        8    0.000    0.000    0.000    0.000 common.py:1148(needs_i8_conversion)\n",
      "       22    0.000    0.000    0.000    0.000 common.py:1240(is_float_dtype)\n",
      "       11    0.000    0.000    0.000    0.000 common.py:1274(is_bool_dtype)\n",
      "    15295    0.003    0.000    0.005    0.000 common.py:1416(is_1d_only_ea_dtype)\n",
      "       11    0.000    0.000    0.000    0.000 common.py:1429(is_extension_array_dtype)\n",
      "       96    0.000    0.000    0.000    0.000 common.py:145(classes)\n",
      "       96    0.000    0.000    0.000    0.000 common.py:147(<lambda>)\n",
      "       32    0.000    0.000    0.000    0.000 common.py:1483(is_ea_or_datetimelike_dtype)\n",
      "       25    0.000    0.000    0.000    0.000 common.py:150(classes_and_not_datetimelike)\n",
      "       25    0.000    0.000    0.000    0.000 common.py:155(<lambda>)\n",
      "       35    0.000    0.000    0.000    0.000 common.py:1552(get_dtype)\n",
      "      121    0.000    0.000    0.000    0.000 common.py:1587(_is_dtype_type)\n",
      "    30491    0.009    0.000    0.013    0.000 common.py:160(cast_scalar_indexer)\n",
      "       30    0.000    0.000    0.000    0.000 common.py:161(is_object_dtype)\n",
      "    15245    0.009    0.000    0.030    0.000 common.py:1721(validate_all_hashable)\n",
      "    30490    0.007    0.000    0.014    0.000 common.py:1740(<genexpr>)\n",
      "       14    0.000    0.000    0.000    0.000 common.py:1747(pandas_dtype)\n",
      "        3    0.000    0.000    0.000    0.000 common.py:208(any_not_none)\n",
      "        3    0.000    0.000    0.000    0.000 common.py:212(<genexpr>)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:222(count_not_none)\n",
      "        3    0.000    0.000    0.000    0.000 common.py:226(<genexpr>)\n",
      "       17    0.004    0.000    0.010    0.001 common.py:229(asarray_tuplesafe)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:254(index_labels_to_array)\n",
      "        3    0.000    0.000    0.000    0.000 common.py:281(maybe_make_list)\n",
      "        2    0.000    0.000    0.000    0.000 common.py:287(maybe_iterable_to_list)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:315(is_datetime64_dtype)\n",
      "    30484    0.007    0.000    0.010    0.000 common.py:346(apply_if_callable)\n",
      "        3    0.000    0.000    0.000    0.000 common.py:348(is_datetime64tz_dtype)\n",
      "       45    0.000    0.000    0.000    0.000 common.py:389(is_timedelta64_dtype)\n",
      "        5    0.000    0.000    0.000    0.000 common.py:459(is_interval_dtype)\n",
      "        9    0.000    0.000    0.000    0.000 common.py:497(is_categorical_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:533(is_string_or_object_np_dtype)\n",
      "    15286    0.006    0.000    0.017    0.000 common.py:552(require_length_match)\n",
      "       12    0.000    0.000    0.000    0.000 common.py:581(is_dtype_equal)\n",
      "        7    0.000    0.000    0.000    0.000 common.py:680(is_integer_dtype)\n",
      "        9    0.000    0.000    0.000    0.000 common.py:732(is_signed_integer_dtype)\n",
      "        2    0.000    0.000    0.000    0.000 common.py:75(get_op_result_name)\n",
      "        9    0.000    0.000    0.000    0.000 common.py:786(is_unsigned_integer_dtype)\n",
      "        2    0.000    0.000    0.000    0.000 common.py:97(_maybe_match_name)\n",
      "        1    0.000    0.000    0.000    0.000 concat.py:106(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 concat.py:111(<setcomp>)\n",
      "        2    0.000    0.000    0.000    0.000 concat.py:112(<genexpr>)\n",
      "        1    0.000    0.000    0.000    0.000 concat.py:117(<setcomp>)\n",
      "        2    0.000    0.000    0.000    0.000 concat.py:118(<genexpr>)\n",
      "        1    0.001    0.001    0.006    0.006 concat.py:185(concatenate_managers)\n",
      "        1    0.000    0.000    0.000    0.000 concat.py:208(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 concat.py:255(_maybe_reindex_columns_na_proxy)\n",
      "        2    0.000    0.000    0.000    0.000 concat.py:287(_get_mgr_concatenation_plan)\n",
      "        1    0.000    0.000    0.000    0.000 concat.py:363(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 concat.py:394(_is_valid_na_for)\n",
      "        1    0.000    0.000    0.000    0.000 concat.py:424(is_na)\n",
      "        1    0.000    0.000    0.003    0.003 concat.py:453(get_reindexed_values)\n",
      "        1    0.000    0.000    0.005    0.005 concat.py:535(_concatenate_join_units)\n",
      "        2    0.000    0.000    0.000    0.000 concat.py:547(<genexpr>)\n",
      "        1    0.000    0.000    0.003    0.003 concat.py:550(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 concat.py:588(_dtype_to_na_value)\n",
      "        1    0.000    0.000    0.000    0.000 concat.py:611(_get_empty_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 concat.py:641(_is_uniform_join_units)\n",
      "        2    0.000    0.000    0.000    0.000 concat.py:653(<genexpr>)\n",
      "        2    0.000    0.000    0.000    0.000 concat.py:656(<genexpr>)\n",
      "        2    0.000    0.000    0.000    0.000 concat.py:666(<genexpr>)\n",
      "        2    0.000    0.000    0.000    0.000 concat.py:669(<genexpr>)\n",
      "        2    0.000    0.000    0.000    0.000 concat.py:711(_combine_concat_plans)\n",
      "        1    0.000    0.000    0.000    0.000 concat.py:74(concat_compat)\n",
      "        2    0.000    0.000    0.000    0.000 concat.py:95(is_nonempty)\n",
      "    15248    0.020    0.000    0.048    0.000 config.py:109(_get_single_key)\n",
      "    15248    0.012    0.000    0.077    0.000 config.py:127(_get_option)\n",
      "    15248    0.007    0.000    0.084    0.000 config.py:255(__call__)\n",
      "    15248    0.004    0.000    0.004    0.000 config.py:571(_select_options)\n",
      "    15248    0.013    0.000    0.017    0.000 config.py:589(_get_root)\n",
      "    30496    0.012    0.000    0.012    0.000 config.py:603(_get_deprecated_option)\n",
      "    15248    0.004    0.000    0.009    0.000 config.py:630(_translate_key)\n",
      "    15248    0.005    0.000    0.013    0.000 config.py:642(_warn_if_deprecated)\n",
      "        5    0.040    0.008    0.316    0.063 construction.py:102(arrays_to_mgr)\n",
      "        3    0.000    0.000    0.295    0.098 construction.py:1051(_convert_object_array)\n",
      "       44    0.000    0.000    0.295    0.007 construction.py:1067(convert)\n",
      "        3    0.000    0.000    0.295    0.098 construction.py:1073(<listcomp>)\n",
      "        6    0.000    0.000    0.000    0.000 construction.py:233(mgr_to_mgr)\n",
      "        1    0.000    0.000    0.000    0.000 construction.py:274(ndarray_to_mgr)\n",
      "    15295    0.007    0.000    0.036    0.000 construction.py:379(extract_array)\n",
      "        1    0.000    0.000    0.000    0.000 construction.py:407(_check_values_indices_shape_match)\n",
      "        2    0.000    0.000    0.004    0.002 construction.py:425(dict_to_mgr)\n",
      "    15269    0.007    0.000    0.008    0.000 construction.py:438(ensure_wrapped_if_datetimelike)\n",
      "    15293    0.051    0.000    0.233    0.000 construction.py:470(sanitize_array)\n",
      "        2    0.000    0.000    0.000    0.000 construction.py:483(<listcomp>)\n",
      "        2    0.000    0.000    0.000    0.000 construction.py:486(<listcomp>)\n",
      "        2    0.000    0.000    0.000    0.000 construction.py:487(<listcomp>)\n",
      "        2    0.000    0.000    0.000    0.000 construction.py:494(<listcomp>)\n",
      "        3    0.000    0.000    1.195    0.398 construction.py:505(nested_data_to_arrays)\n",
      "        3    0.000    0.000    0.000    0.000 construction.py:534(treat_as_nested)\n",
      "        1    0.000    0.000    0.000    0.000 construction.py:549(_prep_ndarray)\n",
      "        5    0.000    0.000    0.036    0.007 construction.py:596(_homogenize)\n",
      "    15293    0.009    0.000    0.018    0.000 construction.py:630(_sanitize_ndim)\n",
      "        2    0.000    0.000    0.000    0.000 construction.py:635(_extract_index)\n",
      "    15293    0.005    0.000    0.006    0.000 construction.py:667(_sanitize_str_dtypes)\n",
      "    15293    0.005    0.000    0.006    0.000 construction.py:687(_maybe_repeat)\n",
      "    15293    0.019    0.000    0.112    0.000 construction.py:698(_try_cast)\n",
      "        1    0.000    0.000    0.000    0.000 construction.py:747(_get_axes)\n",
      "        3    0.012    0.004    1.195    0.398 construction.py:798(to_arrays)\n",
      "    15242    0.008    0.000    0.012    0.000 construction.py:805(is_empty_data)\n",
      "        1    0.000    0.000    0.002    0.002 construction.py:887(_list_to_arrays)\n",
      "        2    0.000    0.000    0.883    0.442 construction.py:934(_list_of_dict_to_arrays)\n",
      "   220812    0.163    0.000    0.175    0.000 construction.py:958(<genexpr>)\n",
      "        4    0.000    0.000    0.000    0.000 construction.py:959(<genexpr>)\n",
      "        2    0.036    0.018    0.036    0.018 construction.py:965(<listcomp>)\n",
      "        3    0.002    0.001    0.297    0.099 construction.py:971(_finalize_columns_and_data)\n",
      "        3    0.000    0.000    0.000    0.000 construction.py:993(_validate_or_indexify_columns)\n",
      "        3    0.000    0.000    0.000    0.000 dataclasses.py:1211(is_dataclass)\n",
      "   117407    0.596    0.000   10.233    0.000 decoder.py:332(decode)\n",
      "   117407    9.476    0.000    9.476    0.000 decoder.py:343(raw_decode)\n",
      "        2    0.000    0.000    0.000    0.000 dtype.py:216(construct_from_string)\n",
      "        2    0.000    0.000    0.000    0.000 dtypes.py:1144(construct_from_string)\n",
      "        5    0.000    0.000    0.000    0.000 dtypes.py:1206(is_dtype)\n",
      "        2    0.000    0.000    0.000    0.000 dtypes.py:300(construct_from_string)\n",
      "        2    0.000    0.000    0.000    0.000 dtypes.py:732(construct_from_string)\n",
      "        2    0.000    0.000    0.000    0.000 dtypes.py:891(construct_from_string)\n",
      "        7    0.000    0.000    0.000    0.000 enum.py:801(value)\n",
      "    15262    0.007    0.000    0.007    0.000 flags.py:47(__init__)\n",
      "       14    0.000    0.000    0.000    0.000 flags.py:51(allows_duplicate_labels)\n",
      "       14    0.000    0.000    0.000    0.000 flags.py:83(allows_duplicate_labels)\n",
      "        1    0.000    0.000    0.000    0.000 frame.py:10314(_get_agg_axis)\n",
      "        1    0.000    0.000    0.001    0.001 frame.py:10817(values)\n",
      "    15240    0.018    0.000    0.726    0.000 frame.py:1279(iterrows)\n",
      "        3    0.000    0.000    0.000    0.000 frame.py:1413(__len__)\n",
      "        1    0.000    0.000    0.001    0.001 frame.py:3273(transpose)\n",
      "        1    0.000    0.000    0.001    0.001 frame.py:3404(T)\n",
      "        5    0.000    0.000    0.002    0.000 frame.py:3411(_ixs)\n",
      "        4    0.000    0.000    0.003    0.001 frame.py:3463(__getitem__)\n",
      "        4    0.000    0.000    0.000    0.000 frame.py:3906(_box_col_values)\n",
      "        5    0.000    0.000    0.000    0.000 frame.py:3920(_clear_item_cache)\n",
      "        4    0.000    0.000    0.002    0.001 frame.py:3923(_get_item_cache)\n",
      "        1    0.000    0.000    0.055    0.055 frame.py:4615(_reindex_axes)\n",
      "        1    0.000    0.000    0.055    0.055 frame.py:4652(_reindex_columns)\n",
      "        2    0.000    0.000    0.056    0.028 frame.py:4788(reindex)\n",
      "        1    0.000    0.000    0.001    0.001 frame.py:4809(drop)\n",
      "        2    0.000    0.000    0.003    0.001 frame.py:5641(reset_index)\n",
      "       10    0.000    0.000    0.000    0.000 frame.py:578(_constructor)\n",
      "        1    0.000    0.000    0.013    0.013 frame.py:5859(isna)\n",
      "       15    0.001    0.000    1.515    0.101 frame.py:587(__init__)\n",
      "        1    0.000    0.000    0.022    0.022 frame.py:5882(dropna)\n",
      "        2    0.000    0.000    0.000    0.000 frame.py:804(axes)\n",
      "        1    0.000    0.000    0.000    0.000 frame.py:821(shape)\n",
      "        1    0.003    0.003    0.043    0.043 frame.py:8236(explode)\n",
      "        1    0.000    0.000    0.000    0.000 frame.py:881(_can_fast_transpose)\n",
      "        1    0.000    0.000    0.000    0.000 frame.py:898(_values)\n",
      "        1    0.002    0.002    0.021    0.021 frame.py:9109(join)\n",
      "        1    0.000    0.000    0.020    0.020 frame.py:9267(_join_compat)\n",
      "        1    0.000    0.000    0.003    0.003 frame.py:9940(_reduce)\n",
      "        1    0.000    0.000    0.000    0.000 frame.py:9987(blk_func)\n",
      "        3    0.000    0.000    0.000    0.000 fromnumeric.py:1008(_argsort_dispatcher)\n",
      "        3    0.000    0.000    0.001    0.000 fromnumeric.py:1012(argsort)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2401(_all_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2406(all)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2922(_prod_dispatcher)\n",
      "        1    0.000    0.000    0.001    0.001 fromnumeric.py:2927(prod)\n",
      "        3    0.000    0.000    0.001    0.000 fromnumeric.py:51(_wrapfunc)\n",
      "        2    0.000    0.000    0.001    0.001 fromnumeric.py:69(_wrapreduction)\n",
      "        2    0.000    0.000    0.000    0.000 fromnumeric.py:70(<dictcomp>)\n",
      "        5    0.000    0.000    0.000    0.000 function.py:49(__call__)\n",
      "        1    0.000    0.000    0.000    0.000 function_base.py:4995(_delete_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 function_base.py:4999(delete)\n",
      "        2    0.000    0.000    0.003    0.002 generic.py:10400(_logical_func)\n",
      "        2    0.000    0.000    0.003    0.002 generic.py:10470(all)\n",
      "        2    0.000    0.000    0.003    0.002 generic.py:10895(all)\n",
      "        1    0.001    0.001    0.002    0.002 generic.py:1516(__invert__)\n",
      "    15262    0.028    0.000    0.035    0.000 generic.py:239(__init__)\n",
      "       14    0.000    0.000    0.000    0.000 generic.py:328(attrs)\n",
      "       28    0.000    0.000    0.000    0.000 generic.py:349(flags)\n",
      "        1    0.000    0.000    0.002    0.002 generic.py:3609(take)\n",
      "        1    0.000    0.000    0.003    0.003 generic.py:3708(_take_with_is_copy)\n",
      "        2    0.000    0.000    0.000    0.000 generic.py:3925(_set_is_copy)\n",
      "        2    0.000    0.000    0.000    0.000 generic.py:4060(_check_inplace_and_allows_duplicate_labels)\n",
      "        1    0.000    0.000    0.001    0.001 generic.py:4238(drop)\n",
      "        1    0.000    0.000    0.001    0.001 generic.py:4274(_drop_axis)\n",
      "    46101    0.011    0.000    0.016    0.000 generic.py:43(_check)\n",
      "        2    0.000    0.000    0.000    0.000 generic.py:441(_validate_dtype)\n",
      "        2    0.000    0.000    0.056    0.028 generic.py:4719(reindex)\n",
      "        4    0.000    0.000    0.000    0.000 generic.py:4952(<genexpr>)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:4997(_needs_reindex_multi)\n",
      "        1    0.000    0.000    0.046    0.046 generic.py:5009(_reindex_with_indexers)\n",
      "        2    0.000    0.000    0.000    0.000 generic.py:514(_construct_axes_from_arguments)\n",
      "        2    0.000    0.000    0.000    0.000 generic.py:543(<dictcomp>)\n",
      "       18    0.000    0.000    0.000    0.000 generic.py:546(_get_axis_number)\n",
      "       16    0.001    0.000    0.001    0.000 generic.py:5517(__finalize__)\n",
      "        4    0.000    0.000    0.000    0.000 generic.py:554(_get_axis_name)\n",
      "    15245    0.011    0.000    0.011    0.000 generic.py:5561(__getattr__)\n",
      "    15261    0.026    0.000    0.099    0.000 generic.py:5577(__setattr__)\n",
      "        8    0.000    0.000    0.000    0.000 generic.py:560(_get_axis)\n",
      "        5    0.000    0.000    0.000    0.000 generic.py:5632(_protect_consolidate)\n",
      "        5    0.000    0.000    0.000    0.000 generic.py:5646(_consolidate_inplace)\n",
      "        5    0.000    0.000    0.000    0.000 generic.py:5650(f)\n",
      "        2    0.000    0.000    0.000    0.000 generic.py:566(_get_block_manager_axis)\n",
      "        1    0.000    0.000    0.001    0.001 generic.py:5718(dtypes)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:5749(astype)\n",
      "        2    0.000    0.000    0.003    0.001 generic.py:5926(copy)\n",
      "        3    0.000    0.000    0.000    0.000 generic.py:636(_info_axis)\n",
      "        5    0.000    0.000    0.000    0.000 generic.py:660(ndim)\n",
      "        1    0.000    0.000    0.001    0.001 generic.py:683(size)\n",
      "        3    0.000    0.000    0.000    0.000 generic.py:767(_set_axis)\n",
      "        1    0.000    0.000    0.003    0.003 indexing.py:1169(_getitem_axis)\n",
      "        1    0.001    0.001    0.001    0.001 indexing.py:130(iloc)\n",
      "        1    0.000    0.000    0.000    0.000 indexing.py:1437(_validate_integer)\n",
      "        1    0.000    0.000    0.000    0.000 indexing.py:1490(_getitem_axis)\n",
      "        1    0.000    0.000    0.000    0.000 indexing.py:2357(check_bool_indexer)\n",
      "    30484    0.017    0.000    0.027    0.000 indexing.py:2486(check_deprecated_indexers)\n",
      "        1    0.000    0.000    0.000    0.000 indexing.py:267(loc)\n",
      "        1    0.000    0.000    0.000    0.000 indexing.py:629(__call__)\n",
      "        2    0.000    0.000    0.003    0.001 indexing.py:954(__getitem__)\n",
      "        1    0.000    0.000    0.003    0.003 indexing.py:981(_getbool_axis)\n",
      "        5    0.000    0.000    0.000    0.000 inference.py:184(is_array_like)\n",
      "        3    0.000    0.000    0.000    0.000 inference.py:262(is_dict_like)\n",
      "        9    0.000    0.000    0.000    0.000 inference.py:288(<genexpr>)\n",
      "        3    0.000    0.000    0.000    0.000 inference.py:294(is_named_tuple)\n",
      "    30510    0.008    0.000    0.012    0.000 inference.py:321(is_hashable)\n",
      "        3    0.001    0.000    0.001    0.000 inference.py:390(is_dataclass)\n",
      "        1    0.000    0.000    0.000    0.000 inference.py:426(is_inferred_bool_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:191(isclass)\n",
      "        1    0.000    0.000    0.002    0.002 interactiveshell.py:277(_modified_open)\n",
      "        4    0.000    0.000    0.000    0.000 managers.py:1026(iget)\n",
      "        1    0.000    0.000    0.001    0.001 managers.py:1376(reduce)\n",
      "        1    0.000    0.000    0.001    0.001 managers.py:1551(as_array)\n",
      "        9    0.000    0.000    0.000    0.000 managers.py:156(blknos)\n",
      "        1    0.000    0.000    0.001    0.001 managers.py:1611(_interleave)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:1625(<listcomp>)\n",
      "       13    0.000    0.000    0.000    0.000 managers.py:1675(is_consolidated)\n",
      "       10    0.000    0.000    0.000    0.000 managers.py:1683(_consolidate_check)\n",
      "        3    0.000    0.000    0.000    0.000 managers.py:1689(<listcomp>)\n",
      "        8    0.003    0.000    0.134    0.017 managers.py:1693(_consolidate_inplace)\n",
      "    15247    0.008    0.000    0.008    0.000 managers.py:1714(__init__)\n",
      "        8    0.000    0.000    0.000    0.000 managers.py:172(blklocs)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:1736(from_blocks)\n",
      "    15242    0.018    0.000    0.136    0.000 managers.py:1745(from_array)\n",
      "    15246    0.003    0.000    0.003    0.000 managers.py:1806(_block)\n",
      "       16    0.000    0.000    0.000    0.000 managers.py:1851(dtype)\n",
      "    30492    0.017    0.000    0.021    0.000 managers.py:1862(internal_values)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:1925(create_block_manager_from_blocks)\n",
      "        5    0.000    0.000    0.241    0.048 managers.py:1951(create_block_manager_from_column_arrays)\n",
      "       46    0.000    0.000    0.000    0.000 managers.py:2006(_grouping_func)\n",
      "        5    0.003    0.001    0.107    0.021 managers.py:2022(_form_blocks)\n",
      "       12    0.089    0.007    0.103    0.009 managers.py:2074(_stack_arrays)\n",
      "        2    0.000    0.000    0.130    0.065 managers.py:2088(_consolidate)\n",
      "       16    0.000    0.000    0.002    0.000 managers.py:2093(<lambda>)\n",
      "        4    0.084    0.021    0.128    0.032 managers.py:2105(_merge_blocks)\n",
      "        3    0.000    0.000    0.000    0.000 managers.py:2116(<listcomp>)\n",
      "        3    0.000    0.000    0.000    0.000 managers.py:212(set_axis)\n",
      "        3    0.000    0.000    0.000    0.000 managers.py:2125(<listcomp>)\n",
      "        2    0.000    0.000    0.000    0.000 managers.py:2151(_preprocess_slice_or_indexer)\n",
      "        5    0.000    0.000    0.000    0.000 managers.py:217(is_single_block)\n",
      "       10    0.000    0.000    0.000    0.000 managers.py:222(items)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:226(get_dtypes)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:227(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:230(arrays)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:239(<listcomp>)\n",
      "        5    0.000    0.000    0.016    0.003 managers.py:253(apply)\n",
      "        5    0.000    0.000    0.000    0.000 managers.py:282(<dictcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:418(astype)\n",
      "        2    0.000    0.000    0.002    0.001 managers.py:578(copy)\n",
      "        4    0.000    0.000    0.000    0.000 managers.py:596(copy_func)\n",
      "        2    0.000    0.000    0.000    0.000 managers.py:599(<listcomp>)\n",
      "        5    0.000    0.000    0.000    0.000 managers.py:618(consolidate)\n",
      "        3    0.000    0.000    0.048    0.016 managers.py:634(reindex_indexer)\n",
      "        1    0.000    0.000    0.002    0.002 managers.py:689(<listcomp>)\n",
      "        2    0.000    0.000    0.045    0.023 managers.py:710(_slice_take_blocks_ax0)\n",
      "        1    0.000    0.000    0.002    0.002 managers.py:872(take)\n",
      "       15    0.000    0.000    0.000    0.000 managers.py:916(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:957(_verify_integrity)\n",
      "        2    0.000    0.000    0.000    0.000 managers.py:959(<genexpr>)\n",
      "        8    0.000    0.000    0.000    0.000 managers.py:970(from_blocks)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:980(fast_xs)\n",
      "        1    0.000    0.000    0.000    0.000 merge.py:1041(_get_merge_keys)\n",
      "        1    0.000    0.000    0.000    0.000 merge.py:1159(_maybe_coerce_merge_keys)\n",
      "        1    0.000    0.000    0.000    0.000 merge.py:1324(_validate_specification)\n",
      "        3    0.000    0.000    0.000    0.000 merge.py:2272(_any)\n",
      "        2    0.000    0.000    0.000    0.000 merge.py:2276(_validate_operand)\n",
      "        1    0.000    0.000    0.000    0.000 merge.py:2290(_items_overlap_with_suffix)\n",
      "        1    0.000    0.000    0.000    0.000 merge.py:611(__init__)\n",
      "        1    0.000    0.000    0.017    0.017 merge.py:712(get_result)\n",
      "        1    0.000    0.000    0.000    0.000 merge.py:746(_maybe_drop_cross_column)\n",
      "        1    0.000    0.000    0.000    0.000 merge.py:796(_maybe_restore_index_levels)\n",
      "        1    0.000    0.000    0.000    0.000 merge.py:832(_maybe_add_join_keys)\n",
      "        1    0.000    0.000    0.017    0.017 merge.py:90(merge)\n",
      "        1    0.000    0.000    0.010    0.010 merge.py:945(_get_join_info)\n",
      "        5    0.000    0.000    0.000    0.000 missing.py:107(clean_fill_method)\n",
      "      2/1    0.000    0.000    0.013    0.013 missing.py:150(_isna)\n",
      "        1    0.000    0.000    0.013    0.013 missing.py:227(_isna_array)\n",
      "        1    0.000    0.000    0.013    0.013 missing.py:268(_isna_string_dtype)\n",
      "        1    0.000    0.000    0.016    0.016 missing.py:288(notna)\n",
      "        7    0.000    0.000    0.001    0.000 missing.py:391(array_equivalent)\n",
      "        4    0.000    0.000    0.001    0.000 missing.py:495(_array_equivalent_object)\n",
      "        4    0.000    0.000    0.000    0.000 missing.py:572(na_value_for_dtype)\n",
      "      2/1    0.000    0.000    0.013    0.013 missing.py:67(isna)\n",
      "        5    0.000    0.000    0.000    0.000 missing.py:911(clean_reindex_fill_method)\n",
      "       59    0.000    0.000    0.000    0.000 multiarray.py:1071(copyto)\n",
      "        7    0.000    0.000    0.000    0.000 multiarray.py:148(concatenate)\n",
      "        2    0.000    0.000    0.000    0.000 nanops.py:191(_get_fill_value)\n",
      "        2    0.000    0.000    0.000    0.000 nanops.py:213(_maybe_get_mask)\n",
      "        2    0.000    0.000    0.000    0.000 nanops.py:257(_get_values)\n",
      "        2    0.000    0.000    0.000    0.000 nanops.py:346(_na_ok_dtype)\n",
      "        2    0.000    0.000    0.000    0.000 nanops.py:534(nanall)\n",
      "        1    0.000    0.000    0.000    0.000 numeric.py:133(__new__)\n",
      "        1    0.000    0.000    0.000    0.000 numeric.py:139(_ensure_array)\n",
      "        1    0.000    0.000    0.000    0.000 numeric.py:188(_validate_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 numeric.py:199(_ensure_dtype)\n",
      "       59    0.000    0.000    0.003    0.000 numeric.py:289(full)\n",
      "        2    0.000    0.000    0.000    0.000 numeric.py:331(_is_all_dates)\n",
      "        6    0.000    0.000    0.000    0.000 numerictypes.py:282(issubclass_)\n",
      "        3    0.000    0.000    0.000    0.000 numerictypes.py:356(issubdtype)\n",
      "        2    0.000    0.000    0.000    0.000 numerictypes.py:573(_can_coerce_all)\n",
      "       19    0.000    0.000    0.000    0.000 numerictypes.py:582(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 numerictypes.py:597(find_common_type)\n",
      "        1    0.000    0.000    0.000    0.000 numerictypes.py:649(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 numerictypes.py:650(<listcomp>)\n",
      "        5    0.000    0.000    0.000    0.000 range.py:167(_simple_new)\n",
      "        2    0.000    0.000    0.000    0.000 range.py:183(_constructor)\n",
      "        2    0.000    0.000    0.000    0.000 range.py:189(_data)\n",
      "        2    0.000    0.000    0.000    0.000 range.py:241(start)\n",
      "        2    0.000    0.000    0.000    0.000 range.py:264(stop)\n",
      "        2    0.000    0.000    0.000    0.000 range.py:287(step)\n",
      "        9    0.000    0.000    0.000    0.000 range.py:347(dtype)\n",
      "        3    0.000    0.000    0.000    0.000 range.py:351(is_unique)\n",
      "        1    0.000    0.000    0.000    0.000 range.py:356(is_monotonic_increasing)\n",
      "        2    0.000    0.000    0.000    0.000 range.py:372(inferred_type)\n",
      "    15240    0.002    0.000    0.002    0.000 range.py:427(__iter__)\n",
      "        2    0.000    0.000    0.000    0.000 range.py:522(equals)\n",
      "       72    0.000    0.000    0.000    0.000 range.py:909(__len__)\n",
      "        7    0.000    0.000    0.000    0.000 re.py:249(compile)\n",
      "        7    0.000    0.000    0.000    0.000 re.py:288(_compile)\n",
      "    30478    0.029    0.000    0.155    0.000 series.py:1052(_get_value)\n",
      "        4    0.000    0.000    0.000    0.000 series.py:1238(_set_as_cached)\n",
      "    15247    0.090    0.000    0.706    0.000 series.py:323(__init__)\n",
      "        1    0.000    0.000    0.017    0.017 series.py:4043(explode)\n",
      "        1    0.000    0.000    0.000    0.000 series.py:4435(_reduce)\n",
      "        2    0.000    0.000    0.000    0.000 series.py:523(_constructor)\n",
      "    15242    0.008    0.000    0.009    0.000 series.py:542(_set_axis)\n",
      "       16    0.000    0.000    0.000    0.000 series.py:575(dtype)\n",
      "    15249    0.017    0.000    0.028    0.000 series.py:590(name)\n",
      "    15244    0.015    0.000    0.045    0.000 series.py:640(name)\n",
      "    30492    0.012    0.000    0.032    0.000 series.py:687(_values)\n",
      "        5    0.000    0.000    0.000    0.000 series.py:743(__len__)\n",
      "        1    0.000    0.000    0.000    0.000 series.py:825(__array__)\n",
      "    30478    0.051    0.000    0.255    0.000 series.py:943(__getitem__)\n",
      "        3    0.000    0.000    0.000    0.000 shape_base.py:207(_arrays_for_stack_dispatcher)\n",
      "        3    0.000    0.000    0.000    0.000 shape_base.py:218(_vhstack_dispatcher)\n",
      "        3    0.000    0.000    0.042    0.014 shape_base.py:222(vstack)\n",
      "        3    0.000    0.000    0.000    0.000 shape_base.py:77(_atleast_2d_dispatcher)\n",
      "        3    0.000    0.000    0.000    0.000 shape_base.py:81(atleast_2d)\n",
      "        2    0.000    0.000    0.000    0.000 string_.py:118(construct_from_string)\n",
      "        8    0.000    0.000    0.047    0.006 take.py:120(_take_nd_ndarray)\n",
      "        8    0.000    0.000    0.000    0.000 take.py:326(_get_take_nd_function)\n",
      "        8    0.000    0.000    0.001    0.000 take.py:554(_take_preprocess_indexer_and_fill_value)\n",
      "        8    0.000    0.000    0.047    0.006 take.py:57(take_nd)\n",
      "        7    0.000    0.000    0.000    0.000 types.py:176(__get__)\n",
      "    30624    0.003    0.000    0.003    0.000 typing.py:1736(cast)\n",
      "        1    0.000    0.000    0.000    0.000 utils.py:249(maybe_convert_indices)\n",
      "        1    0.000    0.000    0.000    0.000 utils.py:457(check_array_indexer)\n",
      "        1    0.000    0.000    0.000    0.000 utils.py:69(is_list_like_indexer)\n",
      "        7    0.000    0.000    0.001    0.000 warnings.py:130(filterwarnings)\n",
      "        7    0.000    0.000    0.000    0.000 warnings.py:181(_add_filter)\n",
      "        7    0.000    0.000    0.000    0.000 warnings.py:437(__init__)\n",
      "        7    0.001    0.000    0.001    0.000 warnings.py:458(__enter__)\n",
      "        7    0.000    0.000    0.000    0.000 warnings.py:477(__exit__)\n",
      "       25    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x00007FFFC0A46CD0}\n",
      "       21    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}\n",
      "    49772    0.049    0.000    0.049    0.000 {built-in method _codecs.utf_8_decode}\n",
      "        1    0.035    0.035    0.035    0.035 {built-in method _collections._count_elements}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _operator.invert}\n",
      "       21    0.000    0.000    0.000    0.000 {built-in method _warnings._filters_mutated}\n",
      "15261/15260    0.007    0.000    0.021    0.000 {built-in method builtins.all}\n",
      "       11    0.000    0.000    0.000    0.000 {built-in method builtins.any}\n",
      "    30489    0.003    0.000    0.003    0.000 {built-in method builtins.callable}\n",
      "        1    0.003    0.003   16.618   16.618 {built-in method builtins.exec}\n",
      "    61459    0.006    0.000    0.006    0.000 {built-in method builtins.getattr}\n",
      "    15311    0.002    0.000    0.002    0.000 {built-in method builtins.hasattr}\n",
      "    30514    0.004    0.000    0.004    0.000 {built-in method builtins.hash}\n",
      "  5217882    0.365    0.000    0.380    0.000 {built-in method builtins.isinstance}\n",
      "    30736    0.003    0.000    0.003    0.000 {built-in method builtins.issubclass}\n",
      "300909/270295    0.037    0.000    0.044    0.000 {built-in method builtins.len}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
      "        4    0.003    0.001    0.005    0.001 {built-in method builtins.sorted}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.sum}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method fromkeys}\n",
      "        1    0.002    0.002    0.002    0.002 {built-in method io.open}\n",
      "        4    0.001    0.000    0.001    0.000 {built-in method numpy.arange}\n",
      "    15328    0.004    0.000    0.004    0.000 {built-in method numpy.array}\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method numpy.asanyarray}\n",
      "   100/99    0.000    0.000    0.000    0.000 {built-in method numpy.asarray}\n",
      "    78/72    0.044    0.001    0.047    0.001 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
      "       88    0.023    0.000    0.023    0.000 {built-in method numpy.empty}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method numpy.geterrobj}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method numpy.seterrobj}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method numpy.zeros}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method pandas._libs.lib.is_bool_array}\n",
      "        2    0.001    0.001    0.001    0.001 {built-in method pandas._libs.lib.is_datetime_array}\n",
      "        1    0.001    0.001    0.001    0.001 {method '__exit__' of '_io._IOBase' objects}\n",
      "        7    0.001    0.000    0.001    0.000 {method '_rebuild_blknos_and_blklocs' of 'pandas._libs.internals.BlockManager' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'add' of 'pandas._libs.internals.BlockPlacement' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'all' of 'numpy.ndarray' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'any' of 'numpy.ndarray' objects}\n",
      "   117489    0.017    0.000    0.017    0.000 {method 'append' of 'list' objects}\n",
      "        3    0.001    0.000    0.001    0.000 {method 'argsort' of 'numpy.ndarray' objects}\n",
      "       10    0.000    0.000    0.000    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'clear' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'copy' of 'dict' objects}\n",
      "        8    0.004    0.001    0.004    0.001 {method 'copy' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "   234814    0.025    0.000    0.025    0.000 {method 'end' of 're.Match' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}\n",
      "        3    0.001    0.000    0.001    0.000 {method 'format' of 'str' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "        1    0.001    0.001    0.001    0.001 {method 'get_indexer' of 'pandas._libs.index.IndexEngine' objects}\n",
      "    30482    0.011    0.000    0.011    0.000 {method 'get_loc' of 'pandas._libs.index.IndexEngine' objects}\n",
      "        7    0.000    0.000    0.000    0.000 {method 'insert' of 'list' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'item' of 'numpy.ndarray' objects}\n",
      "   310220    0.032    0.000    0.032    0.000 {method 'items' of 'dict' objects}\n",
      "   338221    0.028    0.000    0.028    0.000 {method 'keys' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}\n",
      "   234816    0.122    0.000    0.122    0.000 {method 'match' of 're.Pattern' objects}\n",
      "        2    0.000    0.000    0.003    0.001 {method 'max' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'nonzero' of 'numpy.ndarray' objects}\n",
      "       22    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'pop' of 'list' objects}\n",
      "       16    0.000    0.000    0.000    0.000 {method 'ravel' of 'numpy.ndarray' objects}\n",
      "        9    0.003    0.000    0.003    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "        7    0.000    0.000    0.000    0.000 {method 'remove' of 'list' objects}\n",
      "        1    0.001    0.001    0.001    0.001 {method 'repeat' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'search' of 're.Pattern' objects}\n",
      "    15248    0.004    0.000    0.004    0.000 {method 'split' of 'str' objects}\n",
      "   117413    0.027    0.000    0.027    0.000 {method 'startswith' of 'str' objects}\n",
      "   117407    0.059    0.000    0.059    0.000 {method 'strip' of 'str' objects}\n",
      "        3    0.001    0.000    0.001    0.000 {method 'take' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'transpose' of 'numpy.ndarray' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}\n",
      "   117407    0.152    0.000    0.152    0.000 {method 'update' of 'set' objects}\n",
      "       12    0.000    0.000    0.000    0.000 {method 'upper' of 'str' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
      "    15322    0.002    0.000    0.002    0.000 {pandas._libs.algos.ensure_object}\n",
      "       19    0.001    0.000    0.001    0.000 {pandas._libs.algos.ensure_platform_int}\n",
      "        4    0.002    0.000    0.002    0.000 {pandas._libs.algos.take_1d_int64_int64}\n",
      "        1    0.001    0.001    0.001    0.001 {pandas._libs.algos.take_2d_axis0_int64_int64}\n",
      "        3    0.038    0.013    0.038    0.013 {pandas._libs.algos.take_2d_axis0_object_object}\n",
      "        3    0.000    0.000    0.000    0.000 {pandas._libs.internals.get_blkno_placements}\n",
      "        4    0.000    0.000    0.000    0.000 {pandas._libs.lib.array_equivalent_object}\n",
      "        2    0.515    0.258    0.515    0.258 {pandas._libs.lib.dicts_to_array}\n",
      "        2    0.000    0.000    0.000    0.000 {pandas._libs.lib.dtypes_all_equal}\n",
      "        2    0.134    0.067    0.310    0.155 {pandas._libs.lib.fast_unique_multiple_list_gen}\n",
      "    15312    0.085    0.000    0.085    0.000 {pandas._libs.lib.infer_datetimelike_array}\n",
      "      9/8    0.000    0.000    0.000    0.000 {pandas._libs.lib.infer_dtype}\n",
      "        3    0.001    0.000    0.001    0.000 {pandas._libs.lib.is_all_arraylike}\n",
      "       10    0.000    0.000    0.000    0.000 {pandas._libs.lib.is_bool}\n",
      "    30492    0.004    0.000    0.004    0.000 {pandas._libs.lib.is_float}\n",
      "    60967    0.007    0.000    0.007    0.000 {pandas._libs.lib.is_integer}\n",
      "       15    0.002    0.000    0.002    0.000 {pandas._libs.lib.is_iterator}\n",
      "    45845    0.007    0.000    0.007    0.000 {pandas._libs.lib.is_list_like}\n",
      "    30494    0.004    0.000    0.004    0.000 {pandas._libs.lib.is_scalar}\n",
      "        6    0.000    0.000    0.000    0.000 {pandas._libs.lib.item_from_zerodim}\n",
      "       59    0.258    0.004    0.262    0.004 {pandas._libs.lib.maybe_convert_objects}\n",
      "        1    0.002    0.002    0.002    0.002 {pandas._libs.lib.to_object_array_tuples}\n",
      "        1    0.013    0.013    0.013    0.013 {pandas._libs.missing.isnaobj2d}\n",
      "        1    0.016    0.016    0.016    0.016 {pandas._libs.reshape.explode}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cProfile.run('q3_run_inicial(file_path)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memoria total utilizada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113474.8203125"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(memory_usage((q3_run_inicial, (file_path,))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C贸digo no optimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgdsghg\n",
    "\n",
    "# Lista para almacenar los DataFrames individuales\n",
    "data = []\n",
    "lista_keys = []\n",
    "\n",
    "# Abrir el archivo y leer l铆nea por l铆nea\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Crear un diccionariodataframe con una sola columna \"json\" para cada l铆nea\n",
    "        diccionario_df = pd.DataFrame({'json': [line.strip()]})\n",
    "        # Agregar el DataFrame a la lista\n",
    "        data.append(diccionario_df)\n",
    "        lista_keys.extend(list(diccionario_df['json'].apply(json.loads)[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar todos los encabezados 煤nicos para luego formar un dataframe genetal\n",
    "lista_keys_unicas = set(lista_keys)\n",
    "#Crear data frame vac铆o para luego concatenar\n",
    "df_inicial = pd.DataFrame(columns=lista_keys_unicas)\n",
    "#Iterar para concatenar todos los df\n",
    "for df in data: \n",
    "    df= df.reindex(columns=lista_keys_unicas)\n",
    "    df_inicial= pd.concat([df_inicial,df])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EDA_SIMPADE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
