{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llamada del archivo para su procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"D:\\\\Personal\\\\Prueba_Tecnica\\\\LATAM\\\\farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install py-spy\n",
    "#pip install pyspy\n",
    "#pip install memory-profiler\n",
    "#pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "import cProfile\n",
    "from typing import List, Tuple\n",
    "from datetime import datetime\n",
    "from memory_profiler import memory_usage\n",
    "import emoji\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validaciones iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cara del df \n",
    "\n",
    "data = []\n",
    "# Crear conjunto para la claves únicas \n",
    "unique_keys = set()\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        json_data = json.loads(line.strip())\n",
    "        data.append(json_data)\n",
    "        unique_keys.update(json_data.keys())\n",
    "\n",
    "#Crear el dataframe inicial en función de los datos leídos y con columnas nombradas tal como las claves únicas obtenidas\n",
    "df_1 = pd.DataFrame(data).reindex(columns=unique_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Duplicados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La validación de duplicados se realiza sobre la columna ID, dado que esta la que contiene los registros únicos de los twits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['id'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vacíos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.isnull().sum()/len(df_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respecto a esto se identifca que la columna retweetedTweet tiene el 100% de datos nulos, en su momento si para algún punto se consideran eliminar las columna con alto % de vacío se procederá, de lo contrario permanecerá.\n",
    "\n",
    "Por ejemplo, las columans mentioneUsers se podría esperar un alto % de vacíos dado que en todos los comentarios no necesariamente se etiqueta otra cuenta de \"X\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto 1 datos temporales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Función inicial**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consideraciones con las que se creó la función**\n",
    "\n",
    "1. Primero, se carga el archivo JSON utilizando pd.read_json, teniendo en cuenta que está formado por múltiples líneas.\n",
    "\n",
    "2. Se agrega una nueva columna al DataFrame para almacenar las fechas en formato date.\n",
    "\n",
    "3. La columna de usuario se normaliza para extraer los datos pertinentes, ya que contiene información sobre los usuarios que realizaron las interacciones.\n",
    "\n",
    "4. Para reducir el tamaño del DataFrame y optimizar el procesamiento, se crea un DataFrame más pequeño que incluye solo las columnas esenciales, como el ID de la interacción y la fecha, junto con los datos del usuario en columnas separadas.\n",
    "\n",
    "5. Se utiliza groupby para crear una tabla que contenga las fechas, los usuarios y el recuento de interacciones por usuario en cada fecha.\n",
    "\n",
    "6. Se crea una segunda tabla que calcula el total de interacciones por fecha, lo que permite ordenar la tabla por la fecha de manera efectiva.\n",
    "\n",
    "7. Se uner la tabla del punto 5 y 6 y e ordena la tabla resultante de manera que las fechas con mayor actividad aparezcan primero, y dentro de cada fecha, los usuarios más activos se encuentren en la parte superior.\n",
    "\n",
    "8. Se selecciona el primer registro de cada fecha, lo que proporciona el nombre de usuario con la mayor cantidad de interacciones para esa fecha en particular.\n",
    "\n",
    "9. Finalmente, se crea una lista de tuplas que contiene las fechas y los nombres de usuario correspondientes, limitada a los primeros 10 registros para enfocarse en los usuarios más activos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_return_inicial(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    # Leer el archivo JSON y cargarlo en un DataFrame\n",
    "    df = pd.read_json(file_path, lines=True)\n",
    "    \n",
    "    # Modificar la columna de fechas\n",
    "    df['date_tw'] = pd.to_datetime(df['date']).dt.date\n",
    "    \n",
    "    # Normalizar la columna 'user' para obtener datos de usuarios\n",
    "    df_user = json_normalize(df['user'])\n",
    "    \n",
    "    # Unir el DataFrame original con los datos normalizados de usuarios\n",
    "    df = pd.concat([df[['id','date_tw']], df_user.add_prefix('user.')], axis=1)\n",
    "    \n",
    "    # Agrupar por fecha y usuario, contar las interacciones y obtener el total de interacciones por fecha\n",
    "    df_tw_u = df.groupby(['date_tw', 'user.username'])['id'].count().reset_index(name='count_user')\n",
    "    df_tw_ug = df_tw_u.groupby('date_tw')['count_user'].sum().reset_index(name='count_total')\n",
    "    \n",
    "    # Unir los DataFrames y ordenar por conteo total y por usuario\n",
    "    df_tw_u = pd.merge(left=df_tw_u, right=df_tw_ug, how='left', on='date_tw') \\\n",
    "                .sort_values(by=['count_total', 'count_user'], ascending=[False, False])\n",
    "    \n",
    "    # Seleccionar el usuario con más interacciones para cada fecha\n",
    "    df_tw_u_f = df_tw_u.groupby('date_tw').first().reset_index().sort_values(by=['count_total'], ascending=False)\n",
    "    \n",
    "    # Seleccionar las primeras 10 filas y crear una lista de tuplas de fecha y usuario\n",
    "    lista_resultado = list(zip(df_tw_u_f['date_tw'], df_tw_u_f['user.username']))[:10]\n",
    "    \n",
    "    return lista_resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado de ejecución del a función**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_return_inicial(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluación del tiempo de ejecución**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run('q1_return_inicial(file_path)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memoria total utilizada**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ejecutar la función memory_usage((q1_return_inicial, (file_path,))), se obtiene la memoria usada durante la ejecución de la función q1_return_inicial, así que para obtener el valor total de memoria utilizada se realiza la suma de los valores de la lista. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(memory_usage((q1_return_inicial, (file_path,))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Intentos de optimización de la función inicial**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Función 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambios:\n",
    "\n",
    "1. En lugar de cargar todo el archivo JSON de una vez, se lee línea por línea para evitar cargar todo el archivo en memoria al mismo tiempo, lo que reduce el uso de memoria.\n",
    "\n",
    "2. En lugar de usar add_prefix, se asignan los nombres de columnas directamente para mayor claridad.\n",
    "\n",
    "3. El filtro para encontrar el usuario con mayor interaccion por fecha no se hace por método first(), si no que se usa encontrando los índices de estos usuarios y luego filtrando la tabla que contiene la información de fechas nombre de usarios, en esta misma tabla se filtran los 10 registros que pide el ejecicio.\n",
    "\n",
    "4. Se crea un ciclo para la creación de la lista de datos requeridos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_return_v1(file_path: str) -> List[Tuple[datetime.date, str]] :\n",
    "    # Punto 1: Lista para almacenar los datos JSON\n",
    "    data = []\n",
    "\n",
    "    # Punto 2: Conjunto para almacenar los nombres de las claves únicas\n",
    "    unique_keys = set()\n",
    "\n",
    "    # Punto 3: Abrir el archivo JSON y leer línea por línea\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Punto 4: Cargar la línea como un objeto JSON\n",
    "            json_data = json.loads(line.strip())\n",
    "            \n",
    "            # Punto 5: Agregar el objeto JSON a la lista\n",
    "            data.append(json_data)\n",
    "            \n",
    "            # Punto 6: Actualizar el conjunto de claves únicas\n",
    "            unique_keys.update(json_data.keys())\n",
    "\n",
    "    # Punto 7: Crear DataFrame con los datos JSON\n",
    "    df_inicial = pd.DataFrame(data).reindex(columns=unique_keys)\n",
    "\n",
    "    # Punto 8: Modificar la columna de fechas para obtener el formato AAAA-MM-DD\n",
    "    df_inicial['date_tw'] = pd.to_datetime(df_inicial['date']).dt.date\n",
    "    \n",
    "    # Punto 9: Crear un dataframe con los datos expandidos de los usuarios que luego se une al df inicial\n",
    "    df_user = json_normalize(df_inicial['user'])\n",
    "    ## Se asigna el prefijo a los nombres de las columnas para no perder el origen\n",
    "    df_user.columns = ['user.'+col for col in df_user.columns ]\n",
    "    ## Se une el df de usuarios el df inicial y se eliminan los ínidices\n",
    "    df_inicial = pd.concat([df_inicial[['id','date_tw']].copy(),df_user],axis=1).reset_index(drop=True)   \n",
    "    \n",
    "    # Punto 10: Se crean groupby para procesar solo la información de fechas y nombres de usuario, agregando la columna del conteo de interacciones de usuarios\n",
    "    df_tw_u = df_inicial.groupby(['date_tw','user.username'])['id'].count().reset_index(name='count_user')\n",
    "    \n",
    "    # Punto 11: Se crea un df nuevo que contiene solo el total de interacciones por fecha.\n",
    "    df_tw_ug = df_tw_u.groupby('date_tw')['count_user'].sum().sort_values(ascending=False).reset_index(name='count_total')\n",
    "    ## Se unen el df original de fechas, usuarios, conteo de interacciones por usuario al df que contiene las fechas e interacciones por fecha, se ordena de tal forma que el conteo total y por usuario es descendente\n",
    "    df_tw_u = pd.merge(left=df_tw_u, right=df_tw_ug, how='left', on='date_tw').sort_values(by=['count_total','count_user'],ascending=[False,False])\n",
    "    ## Se buscan los índices de la primera fila de cada fecha.\n",
    "    indeices_max = df_tw_u.groupby('date_tw')['count_user'].idxmax()\n",
    "    ## Se filtra del df de twits de tal forma que se ordene de la fecha de mayor interaccion a la menor, solo los primeros 10 registros\n",
    "    df_tw_u_f = df_tw_u.loc[indeices_max].sort_values(by=['count_total'], ascending=False).iloc[0:10,::].reset_index(drop=True)\n",
    "    \n",
    "    # Punto 12: Se crea un ciclo para llenar una lista con la tuplas de fecha y el usuario con mayor cantidad de interacciones en X para ese día \n",
    "    lista_resultado = []\n",
    "    for i in range(len(df_tw_u_f)):\n",
    "        lista_resultado.append((df_tw_u_f['date_tw'][i],df_tw_u_f['user.username'][i]))\n",
    "    \n",
    "    return lista_resultado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado de ejecución del a función**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_return_v1(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluación del tiempo de ejecución**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run('q1_return_v1(file_path)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memoria total utilizada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(memory_usage((q1_return_v1, (file_path,))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Función 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambios:\n",
    "\n",
    "Si bien se mantiene una estructura muy similar a la de la función 1, se realizan algunos pequeños cambios:\n",
    "\n",
    "1. Al crear el concat entre el DataFrame inicila y el DataFrame de usuarios, en el segundo caso se usa solo la columna objetivo, que es el nombre de usuario.\n",
    "2. La lista de resultados se realiza con una lista de compresión en iteración sobre las líneas del df final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_return_t2(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    data = []\n",
    "    unique_keys = set()\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            json_data = json.loads(line.strip())\n",
    "            data.append(json_data)\n",
    "            unique_keys.update(json_data.keys())\n",
    "\n",
    "    df_inicial = pd.DataFrame(data).reindex(columns=unique_keys)\n",
    "    df_inicial['date_tw'] = pd.to_datetime(df_inicial['date']).dt.date\n",
    "\n",
    "    df_user = json_normalize(df_inicial['user'])\n",
    "    df_user.columns = ['user.'+col for col in df_user.columns]\n",
    "    df_inicial = pd.concat([df_inicial[['id','date_tw']], df_user['user.username']], axis=1).reset_index(drop=True)\n",
    "\n",
    "    df_tw_u = df_inicial.groupby(['date_tw', 'user.username'])['id'].count().reset_index(name='count_user')\n",
    "    df_tw_ug = df_tw_u.groupby('date_tw')['count_user'].sum().reset_index(name='count_total')\n",
    "\n",
    "    df_tw_u = pd.merge(left=df_tw_u, right=df_tw_ug, how='left', on='date_tw').sort_values(by=['count_total', 'count_user'], ascending=[False, False])\n",
    "\n",
    "    indices_max = df_tw_u.groupby('date_tw')['count_user'].idxmax()\n",
    "    df_tw_u_f = df_tw_u.loc[indices_max].sort_values(by=['count_total'], ascending=False).iloc[0:10].reset_index(drop=True)\n",
    "\n",
    "    lista_resultado = [(row['date_tw'], row['user.username']) for _, row in df_tw_u_f.iterrows()]\n",
    "\n",
    "    return lista_resultado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado de ejecución del a función**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_return_t2(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluación del tiempo de ejecución**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run('q1_return_t2(file_path)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memoria total utilizada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(memory_usage((q1_return_t2, (file_path,))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto 2 emojies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Función inicial**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consideraciones con las que se creó la función**\n",
    "\n",
    "1. Primero, se carga el archivo JSON utilizando pd.read_json, teniendo en cuenta que está formado por múltiples líneas. La forma de carga adoptada tiene el sentido de no sobrecargar la memoria con una carga completa.\n",
    "\n",
    "2. Las claves únicas tiene el sentido de asegurar que al conformar el dataframe luego de leer los datos van a estar encabezados de forma adecuada.\n",
    "\n",
    "3. Se realiza el uso de la librería emoji y dentro de ella el atriburo EMOJI_DATA, el cual contiene la información asociada a los emojies.\n",
    "\n",
    "4. Se crea una función para extraer los emojies de cada fila de contenido, de esta manera crea una lista de emojies para cada fila de content.\n",
    "\n",
    "5. Se crea una columna dentro del dataFrame para poder almancenas los emojies encontrados en cada fila del content.\n",
    "\n",
    "6. Se crea una lista de vacía de emojies a la cual se va integrando con las listas de los emojies encontrados en cada fila de la colimna 'emojis'\n",
    "\n",
    "7. Se crea un dataframe con la lista de los emojies, luego se crea un nuevo df sobre este luego de hacer el conteo descente de aparición para cada emojie.\n",
    "\n",
    "8. Se filtran los datos de color de los emojies, dado que por el momento, el enfoque se centra en identificar la forma básica de los emojis y no en considerar sus modificadores de color. Esto puede ayudar a evitar confusiones sobre el alcance y los objetivos del análisis actual.\n",
    "\n",
    "9. Finalmente, se crea una lista de tuplas que contiene los emojies y la cantidad de apariciones, limitada a los primeros 10 registros para enfocarse en los usuarios más activos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_run_inicial(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Leer el archivo JSON y cargarlo en un DataFrame\n",
    "    data = []\n",
    "    # Crear conjunto para la claves únicas \n",
    "    unique_keys = set()\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            json_data = json.loads(line.strip())\n",
    "            data.append(json_data)\n",
    "            unique_keys.update(json_data.keys())\n",
    "\n",
    "    #Crear el dataframe inicial en función de los datos leídos y con columnas nombradas tal como las claves únicas obtenidas\n",
    "    df_inicial = pd.DataFrame(data).reindex(columns=unique_keys)\n",
    "    \n",
    "    #Usando la librería de emoji se crea una lista con los emojies \"c\" dentro un texto que se pasa, a la función\n",
    "    def extraer_emojis(texto):\n",
    "        return [c for c in texto if c in emoji.EMOJI_DATA]\n",
    "\n",
    "    #Se aplica la función sobre la columna de contenido y se crea una nueva con el fin de procesar los emojies posteriormente\n",
    "    df_inicial['emojis'] = df_inicial['content'].apply(extraer_emojis)\n",
    "    \n",
    "    lista_emojies = []\n",
    "\n",
    "    #Se crea un ciclo para obtener todos los emojies en una sola lista.\n",
    "    for fila in df_inicial['emojis']:\n",
    "        lista_emojies.extend(fila)\n",
    "        \n",
    "    df_emojies = pd.DataFrame(lista_emojies,columns=['emojie'])\n",
    "    \n",
    "    df_ef = pd.DataFrame(df_emojies.value_counts(),columns=['Cuenta']).reset_index()\n",
    "    \n",
    "    #Se filtran los símbolos que representan la modificación de color de los emojies, se crea esta función bajo el supuesto que por ahora solo se necesita la figura del emojie no su color.\n",
    "    df_ef = df_ef[~df_ef['emojie'].isin(['🏻','🏽','🏼','🏾','🏿'])][0:10]\n",
    "    \n",
    "    lista_resultado = [(row['emojie'], row['Cuenta']) for _, row in df_ef.iterrows()]\n",
    "\n",
    "    return lista_resultado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado de ejecución del a función**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_run_inicial(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluación del tiempo de ejecución**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run('q2_run_inicial(file_path)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memoria total utilizada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(memory_usage((q2_run_inicial, (file_path,))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Intentos de optimización de la función inicial**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Función 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_run_v1(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Leer el archivo JSON y cargarlo en un DataFrame\n",
    "    data = []\n",
    "    # Crear conjunto para la claves únicas \n",
    "    unique_keys = set()\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            json_data = json.loads(line.strip())\n",
    "            data.append(json_data)\n",
    "            unique_keys.update(json_data.keys())\n",
    "\n",
    "    #Crear el dataframe inicial en función de los datos leídos y con columnas nombradas tal como las claves únicas obtenidas\n",
    "    df_inicial = pd.DataFrame(data).reindex(columns=unique_keys)\n",
    "    \n",
    "    #Usando la librería de emoji se crea una lista con los emojies \"c\" dentro un texto que se pasa, a la función\n",
    "    def extraer_emojis(texto):\n",
    "        return [c for c in texto if c in emoji.EMOJI_DATA]\n",
    "\n",
    "    #Se aplica la función sobre la columna de contenido y se crea una nueva con el fin de procesar los emojies posteriormente\n",
    "    df_inicial['emojis'] = df_inicial['content'].apply(extraer_emojis)\n",
    "    \n",
    "    lista_emojies = []\n",
    "\n",
    "    #Se crea un ciclo para obtener todos los emojies en una sola lista.\n",
    "    for fila in df_inicial['emojis']:\n",
    "        lista_emojies.extend(fila)\n",
    "\n",
    "    df_ef = pd.DataFrame(pd.DataFrame(lista_emojies,columns=['emojie']).value_counts(),columns=['Cuenta']).reset_index()\n",
    "    \n",
    "    #Se filtran los símbolos que representan la modificación de color de los emojies, se crea esta función bajo el supuesto que por ahora solo se necesita la figura del emojie no su color.\n",
    "    df_ef = df_ef[~df_ef['emojie'].isin(['🏻','🏽','🏼','🏾','🏿'])][0:10]\n",
    "    \n",
    "    lista_resultado = [(row['emojie'], row['Cuenta']) for _, row in df_ef.iterrows()]\n",
    "\n",
    "    return lista_resultado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado de ejecución del a función**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_run_v1(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluación del tiempo de ejecución**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run('q2_run_v1(file_path)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memoria total utilizada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(memory_usage((q2_run_v1, (file_path,))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_run_v2(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Leer el archivo JSON y cargarlo en un DataFrame\n",
    "    data = []\n",
    "    # Crear conjunto para la claves únicas \n",
    "    unique_keys = set()\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            json_data = json.loads(line.strip())\n",
    "            data.append(json_data)\n",
    "            unique_keys.update(json_data.keys())\n",
    "\n",
    "    #Crear el dataframe inicial en función de los datos leídos y con columnas nombradas tal como las claves únicas obtenidas\n",
    "    df_inicial = pd.DataFrame(data).reindex(columns=unique_keys)\n",
    "    \n",
    "    #Usando la librería de emoji se crea una lista con los emojies \"c\" dentro un texto que se pasa, a la función\n",
    "    def extraer_emojis(texto):\n",
    "        return [c for c in texto if c in emoji.EMOJI_DATA]\n",
    "\n",
    "    #Se aplica la función sobre la columna de contenido y se crea una nueva con el fin de procesar los emojies posteriormente\n",
    "    df_inicial['emojis'] = df_inicial['content'].apply(extraer_emojis)\n",
    "    \n",
    "    # Crear una lista plana con todos los emojis encontrados en todas las filas\n",
    "    lista_emojies = [emoji for fila in df_inicial['emojis'] for emoji in fila]\n",
    "    \n",
    "    # Crear DataFrame con los emojis y su frecuencia\n",
    "    df_ef = pd.DataFrame(Counter(lista_emojies).most_common(), columns=['emojie', 'Cuenta'])\n",
    "        \n",
    "    #Se filtran los símbolos que representan la modificación de color de los emojies, se crea esta función bajo el supuesto que por ahora solo se necesita la figura del emojie no su color.\n",
    "    df_ef = df_ef[~df_ef['emojie'].isin(['🏻','🏽','🏼','🏾','🏿'])][0:10]\n",
    "    \n",
    "    lista_resultado = [(row['emojie'], row['Cuenta']) for _, row in df_ef.iterrows()]\n",
    "\n",
    "    return lista_resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado de ejecución del a función**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_run_v2(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluación del tiempo de ejecución**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run('q2_run_v2(file_path)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memoria total utilizada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(memory_usage((q2_run_v2, (file_path,))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto 3 influyentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Análisis de mención a los usuarios**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se evidencia que tanto en la columna content como en la columna mentionedUsers hat mención a los usuarios, por tanto, es necesario realizar la exploración aleatoria de si los usuarios mencionados en content con los mismo de mentionedUsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qinicial(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Leer el archivo JSON y cargarlo en un DataFrame\n",
    "    data = []\n",
    "    # Crear conjunto para la claves únicas \n",
    "    unique_keys = set()\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            json_data = json.loads(line.strip())\n",
    "            data.append(json_data)\n",
    "            unique_keys.update(json_data.keys())\n",
    "\n",
    "    #Crear el dataframe inicial en función de los datos leídos y con columnas nombradas tal como las claves únicas obtenidas\n",
    "    df_inicial = pd.DataFrame(data).reindex(columns=unique_keys)\n",
    "\n",
    "    return df_inicial\n",
    "\n",
    "resultado2 = qinicial(file_path)[['content','mentionedUsers']].copy().explode(column = 'mentionedUsers')\n",
    "resultado2= resultado2[~resultado2['mentionedUsers'].isnull()].reset_index()\n",
    "pd.set_option('display.max_colwidth', 500) \n",
    "\n",
    "display(resultado2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al observar el caso anterior se deja en evidencia casos como los de los registros de las fila 117388, el cual tiene indicados dos veces a la misma persona, en este caso se debería poder entender la necesidad del cliente final, dado que una peersona comentada dos veces en un mismo twit puede indicar que la persona es relevante para quien actual en \"X\", pero al tiempo podría mostrar un dato desviado, dado que si solo queremos conocer a quien se han etiquetado por comentario, pues no importa su número de veces etiquetado, se deberá contar una única vez.\n",
    "\n",
    "Por el momento, aludiendo a que si hay una etiqueta múltiple para una misma cuenta en un solo comentario podría indicar un sentido de relevancia para quien redacta el mensaje, se trasladará esa urgencia al análisis de los más etiquetados, de tal forma que se obseve en la lista la urgencia o importancia que le dan los usuario de \"X\" a quienes etiquetan. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Función inicial**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado la explicación anterior se va a hacer uso de la columna mentionedUsers para obtener la información de la mensión de usuarios dentro del contenido de una publicación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_run_inicial(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Leer el archivo JSON y cargarlo en un DataFrame\n",
    "    data = []\n",
    "    # Crear conjunto para la claves únicas \n",
    "    unique_keys = set()\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            json_data = json.loads(line.strip())\n",
    "            data.append(json_data)\n",
    "            unique_keys.update(json_data.keys())\n",
    "\n",
    "    #Crear el dataframe inicial en función de los datos leídos y con columnas nombradas tal como las claves únicas obtenidas\n",
    "    df_inicial = pd.DataFrame(data).reindex(columns=unique_keys)\n",
    "    \n",
    "    #Dado que se ha validado previamente que no hay twits duplicados, se puede usar la columna mentionedUsers, para ser explotada (explode) y luego expandida (json_normalize) y con esta hacer todo el procesamiento solicitado.\n",
    "    \n",
    "    df_relevantes =  json_normalize(pd.DataFrame(df_inicial['mentionedUsers']).dropna().explode(column='mentionedUsers').reset_index(drop=True)['mentionedUsers'])\n",
    "    \n",
    "    # Crear DataFrame con los emojis y su frecuencia\n",
    "    df_relev = pd.DataFrame(Counter(df_relevantes['username']).most_common(), columns=['username', 'Cuenta'])\n",
    "    \n",
    "    lista_resultado = [(row['username'], row['Cuenta']) for _, row in df_relev.iterrows()][:10]\n",
    "    \n",
    "    return lista_resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado de ejecución del a función**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 2265),\n",
       " ('Kisanektamorcha', 1840),\n",
       " ('RakeshTikaitBKU', 1644),\n",
       " ('PMOIndia', 1427),\n",
       " ('RahulGandhi', 1146),\n",
       " ('GretaThunberg', 1048),\n",
       " ('RaviSinghKA', 1019),\n",
       " ('rihanna', 986),\n",
       " ('UNHumanRights', 962),\n",
       " ('meenaharris', 926)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_run_inicial(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluación del tiempo de ejecución**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         9856895 function calls (9722860 primitive calls) in 16.618 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    1.107    1.107   15.906   15.906 1861183408.py:1(q3_run_inicial)\n",
      "        1    0.021    0.021    1.002    1.002 1861183408.py:23(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(all)\n",
      "        3    0.000    0.000    0.001    0.000 <__array_function__ internals>:177(argsort)\n",
      "        3    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(atleast_2d)\n",
      "        7    0.000    0.000    0.042    0.006 <__array_function__ internals>:177(concatenate)\n",
      "       59    0.000    0.000    0.003    0.000 <__array_function__ internals>:177(copyto)\n",
      "        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(delete)\n",
      "        1    0.000    0.000    0.001    0.001 <__array_function__ internals>:177(prod)\n",
      "        3    0.000    0.000    0.042    0.014 <__array_function__ internals>:177(vstack)\n",
      "       16    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:1053(_handle_fromlist)\n",
      "        1    0.709    0.709   16.615   16.615 <string>:1(<module>)\n",
      "   117407    0.116    0.000   10.386    0.000 __init__.py:299(loads)\n",
      "        1    0.000    0.000    0.035    0.035 __init__.py:565(__init__)\n",
      "        1    0.000    0.000    0.003    0.003 __init__.py:588(most_common)\n",
      "        1    0.000    0.000    0.035    0.035 __init__.py:640(update)\n",
      "       12    0.000    0.000    0.000    0.000 _asarray.py:109(<setcomp>)\n",
      "       12    0.001    0.000    0.001    0.000 _asarray.py:22(require)\n",
      "        1    0.000    0.000    0.000    0.000 _collections_abc.py:315(__subclasshook__)\n",
      "        5    0.000    0.000    0.000    0.000 _decorators.py:214(_format_argument_list)\n",
      "        5    0.000    0.000    0.025    0.005 _decorators.py:302(wrapper)\n",
      "        2    0.000    0.000    0.056    0.028 _decorators.py:322(wrapper)\n",
      "       16    0.000    0.000    0.000    0.000 _dtype.py:24(_kind_name)\n",
      "       16    0.000    0.000    0.000    0.000 _dtype.py:330(_name_includes_bit_suffix)\n",
      "       16    0.000    0.000    0.001    0.000 _dtype.py:344(_name_get)\n",
      "        2    0.001    0.000    0.003    0.001 _methods.py:38(_amax)\n",
      "        1    0.000    0.000    0.000    0.000 _methods.py:46(_sum)\n",
      "        3    0.000    0.000    0.000    0.000 _methods.py:54(_any)\n",
      "        2    0.000    0.000    0.000    0.000 _methods.py:60(_all)\n",
      "   103403    0.040    0.000    0.052    0.000 _normalize.py:122(_normalise_json)\n",
      "   103403    0.201    0.000    1.218    0.000 _normalize.py:163(_normalise_json_ordered)\n",
      "   103403    0.436    0.000    0.580    0.000 _normalize.py:178(<dictcomp>)\n",
      "   103403    0.219    0.000    0.359    0.000 _normalize.py:180(<dictcomp>)\n",
      " 103404/1    0.053    0.000    1.323    1.323 _normalize.py:188(_simple_json_normalize)\n",
      "        1    0.047    0.047    1.323    1.323 _normalize.py:236(<listcomp>)\n",
      "        1    0.076    0.076    1.900    1.900 _normalize.py:241(_json_normalize)\n",
      "        2    0.000    0.000    0.000    0.000 _ufunc_config.py:131(geterr)\n",
      "        2    0.000    0.000    0.000    0.000 _ufunc_config.py:32(seterr)\n",
      "        1    0.000    0.000    0.000    0.000 _ufunc_config.py:425(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 _ufunc_config.py:429(__enter__)\n",
      "        1    0.000    0.000    0.000    0.000 _ufunc_config.py:434(__exit__)\n",
      "        2    0.000    0.000    0.000    0.000 _validators.py:122(_check_for_invalid_keys)\n",
      "        2    0.000    0.000    0.000    0.000 _validators.py:135(validate_kwargs)\n",
      "        1    0.000    0.000    0.000    0.000 _validators.py:162(validate_args_and_kwargs)\n",
      "        6    0.000    0.000    0.000    0.000 _validators.py:218(validate_bool_kwarg)\n",
      "        1    0.000    0.000    0.000    0.000 _validators.py:23(_check_arg_length)\n",
      "        2    0.000    0.000    0.000    0.000 _validators.py:258(validate_axis_style_args)\n",
      "        2    0.000    0.000    0.000    0.000 _validators.py:43(_check_for_default_values)\n",
      "       21    0.000    0.000    0.000    0.000 abc.py:117(__instancecheck__)\n",
      "        1    0.000    0.000    0.000    0.000 abc.py:121(__subclasscheck__)\n",
      "        2    0.000    0.000    0.001    0.000 algorithms.py:1356(take)\n",
      "        2    0.000    0.000    0.000    0.000 api.py:177(union_indexes)\n",
      "        5    0.000    0.000    0.000    0.000 api.py:331(default_index)\n",
      "        2    0.000    0.000    0.001    0.000 base.py:1106(take)\n",
      "        2    0.000    0.000    0.000    0.000 base.py:1132(_maybe_disallow_fill)\n",
      "        1    0.000    0.000    0.001    0.001 base.py:1194(repeat)\n",
      "        1    0.000    0.000    0.013    0.013 base.py:143(isna)\n",
      "       21    0.000    0.000    0.000    0.000 base.py:1658(name)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:1712(_set_names)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:1741(set_names)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:1852(rename)\n",
      "        3    0.000    0.000    0.000    0.000 base.py:1906(nlevels)\n",
      "        1    0.000    0.000    0.010    0.010 base.py:207(join)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:210(interleaved_dtype)\n",
      "        2    0.000    0.000    0.002    0.001 base.py:2162(is_monotonic)\n",
      "        1    0.002    0.002    0.002    0.002 base.py:2170(is_monotonic_increasing)\n",
      "        6    0.008    0.001    0.010    0.002 base.py:2240(is_unique)\n",
      "        4    0.000    0.000    0.000    0.000 base.py:2280(is_boolean)\n",
      "       22    0.000    0.000    0.000    0.000 base.py:229(construct_from_string)\n",
      "        9    0.000    0.000    0.000    0.000 base.py:229(disallow_kwargs)\n",
      "    30482    0.008    0.000    0.009    0.000 base.py:2352(is_floating)\n",
      "        8    0.000    0.000    0.000    0.000 base.py:2604(inferred_type)\n",
      "        2    0.000    0.000    0.001    0.001 base.py:2611(_is_all_dates)\n",
      "       11    0.000    0.000    0.000    0.000 base.py:2632(_is_multi)\n",
      "        2    0.000    0.000    0.000    0.000 base.py:2650(_na_value)\n",
      "       17    0.000    0.000    0.000    0.000 base.py:286(is_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:3058(_validate_sort_keyword)\n",
      "       14    0.000    0.000    0.000    0.000 base.py:326(ndim)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:3282(intersection)\n",
      "        1    0.004    0.004    0.004    0.004 base.py:329(_left_indexer)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:3562(_assert_can_do_setop)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:3568(_convert_can_do_setop)\n",
      "    30482    0.023    0.000    0.072    0.000 base.py:3585(get_loc)\n",
      "        3    0.000    0.000    0.002    0.001 base.py:3714(get_indexer)\n",
      "        1    0.000    0.000    0.001    0.001 base.py:3794(_get_indexer)\n",
      "        3    0.000    0.000    0.000    0.000 base.py:3837(_check_indexing_method)\n",
      "     16/9    0.000    0.000    0.018    0.002 base.py:397(__new__)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:4106(_validate_can_reindex)\n",
      "        1    0.000    0.000    0.009    0.009 base.py:4123(reindex)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:4240(_wrap_reindex_result)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:4244(_maybe_preserve_names)\n",
      "        1    0.000    0.000    0.010    0.010 base.py:4328(join)\n",
      "        5    0.000    0.000    0.000    0.000 base.py:45(__len__)\n",
      "        3    0.000    0.000    0.000    0.000 base.py:465(find)\n",
      "        1    0.000    0.000    0.004    0.004 base.py:4730(_join_monotonic)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:4776(_wrap_joined_index)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:4788(_can_use_libjoin)\n",
      "       85    0.000    0.000    0.000    0.000 base.py:4834(_values)\n",
      "       10    0.000    0.000    0.000    0.000 base.py:4860(_get_engine_target)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:4871(_from_join_target)\n",
      "        4    0.000    0.000    0.000    0.000 base.py:4987(__contains__)\n",
      "       10    0.001    0.000    0.001    0.000 base.py:5037(__getitem__)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:5106(append)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:5131(<setcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:5136(_concat)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:5140(<listcomp>)\n",
      "        8    0.000    0.000    0.000    0.000 base.py:518(<genexpr>)\n",
      "        8    0.000    0.000    0.001    0.000 base.py:5192(equals)\n",
      "        2    0.000    0.000    0.000    0.000 base.py:5279(identical)\n",
      "        2    0.000    0.000    0.000    0.000 base.py:5292(<genexpr>)\n",
      "       16    0.000    0.000    0.000    0.000 base.py:53(shape)\n",
      "        9    0.000    0.000    0.000    0.000 base.py:540(_ensure_array)\n",
      "       48    0.000    0.000    0.000    0.000 base.py:55(<genexpr>)\n",
      "        9    0.000    0.000    0.000    0.000 base.py:554(_dtype_to_subclass)\n",
      "    30478    0.020    0.000    0.055    0.000 base.py:5660(_get_values_for_loc)\n",
      "        3    0.000    0.000    0.000    0.000 base.py:57(_validate_set_axis)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:5758(get_indexer_for)\n",
      "        6    0.000    0.000    0.006    0.001 base.py:5919(_index_as_unique)\n",
      "        3    0.000    0.000    0.000    0.000 base.py:5931(_maybe_promote)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:5977(_find_common_type_compat)\n",
      "        2    0.000    0.000    0.000    0.000 base.py:6018(_should_compare)\n",
      "        2    0.000    0.000    0.000    0.000 base.py:6036(_is_comparable_dtype)\n",
      "    30482    0.017    0.000    0.038    0.000 base.py:6298(_maybe_cast_indexer)\n",
      "        3    0.000    0.000    0.000    0.000 base.py:6307(_maybe_cast_listlike_indexer)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:6535(delete)\n",
      "       20    0.000    0.000    0.000    0.000 base.py:654(_simple_new)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:6632(drop)\n",
      "        7    0.000    0.000    0.020    0.003 base.py:672(_with_infer)\n",
      "        2    0.000    0.000    0.000    0.000 base.py:692(_constructor)\n",
      "    15269    0.004    0.000    0.028    0.000 base.py:7004(ensure_index)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:7063(ensure_has_len)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:7094(_validate_join_method)\n",
      "    15259    0.006    0.000    0.012    0.000 base.py:7099(maybe_extract_name)\n",
      "        9    0.001    0.000    0.006    0.001 base.py:7123(_maybe_cast_data_without_dtype)\n",
      "        2    0.000    0.000    0.000    0.000 base.py:7185(unpack_nested_dtype)\n",
      "        5    0.000    0.000    0.000    0.000 base.py:744(__iter__)\n",
      "        5    0.000    0.000    0.000    0.000 base.py:785(_view)\n",
      "        8    0.000    0.000    0.000    0.000 base.py:803(is_)\n",
      "       25    0.000    0.000    0.000    0.000 base.py:834(_reset_identity)\n",
      "        7    0.001    0.000    0.001    0.000 base.py:845(_engine)\n",
      "    30537    0.007    0.000    0.010    0.000 base.py:884(__len__)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:890(__array__)\n",
      "       12    0.000    0.000    0.000    0.000 base.py:945(dtype)\n",
      "        4    0.000    0.000    0.000    0.000 base.py:982(view)\n",
      "        3    0.000    0.000    0.042    0.014 blocks.py:1116(take_nd)\n",
      "        8    0.001    0.000    0.002    0.000 blocks.py:166(_consolidate_key)\n",
      "        1    0.000    0.000    0.000    0.000 blocks.py:178(_can_hold_na)\n",
      "    15252    0.015    0.000    0.027    0.000 blocks.py:1962(maybe_coerce_values)\n",
      "    15263    0.016    0.000    0.021    0.000 blocks.py:1991(get_block_type)\n",
      "        1    0.000    0.000    0.000    0.000 blocks.py:201(is_bool)\n",
      "        5    0.000    0.000    0.000    0.000 blocks.py:2032(new_block_2d)\n",
      "    15246    0.028    0.000    0.101    0.000 blocks.py:2043(new_block)\n",
      "    15246    0.014    0.000    0.022    0.000 blocks.py:2057(check_ndim)\n",
      "        9    0.000    0.000    0.000    0.000 blocks.py:2121(extend_blocks)\n",
      "        2    0.000    0.000    0.000    0.000 blocks.py:222(get_values)\n",
      "        3    0.000    0.000    0.000    0.000 blocks.py:238(fill_value)\n",
      "       11    0.000    0.000    0.000    0.000 blocks.py:244(mgr_locs)\n",
      "        4    0.000    0.000    0.000    0.000 blocks.py:252(make_block)\n",
      "        3    0.000    0.000    0.000    0.000 blocks.py:267(make_block_same_class)\n",
      "        3    0.000    0.000    0.000    0.000 blocks.py:350(shape)\n",
      "       19    0.000    0.000    0.000    0.000 blocks.py:354(dtype)\n",
      "        5    0.000    0.000    0.000    0.000 blocks.py:358(iget)\n",
      "        2    0.000    0.000    0.013    0.007 blocks.py:396(apply)\n",
      "        1    0.000    0.000    0.000    0.000 blocks.py:406(reduce)\n",
      "        2    0.000    0.000    0.000    0.000 blocks.py:427(_split_op_result)\n",
      "        1    0.000    0.000    0.000    0.000 blocks.py:560(astype)\n",
      "        2    0.000    0.000    0.002    0.001 blocks.py:638(copy)\n",
      "        1    0.000    0.000    0.000    0.000 cast.py:1199(astype_array)\n",
      "        1    0.000    0.000    0.000    0.000 cast.py:1246(astype_array_safe)\n",
      "    15312    0.033    0.000    0.126    0.000 cast.py:1466(maybe_infer_to_datetimelike)\n",
      "       44    0.001    0.000    0.038    0.001 cast.py:1579(maybe_cast_to_datetime)\n",
      "        8    0.000    0.000    0.000    0.000 cast.py:1711(sanitize_to_nanoseconds)\n",
      "        2    0.000    0.000    0.000    0.000 cast.py:1789(find_common_type)\n",
      "        3    0.000    0.000    0.000    0.000 cast.py:1819(<genexpr>)\n",
      "        2    0.000    0.000    0.000    0.000 cast.py:1828(<genexpr>)\n",
      "        2    0.000    0.000    0.000    0.000 cast.py:1830(<genexpr>)\n",
      "        3    0.000    0.000    0.000    0.000 cast.py:1835(<genexpr>)\n",
      "        7    0.002    0.000    0.006    0.001 cast.py:1962(construct_1d_object_array_from_listlike)\n",
      "        6    0.001    0.000    0.001    0.000 cast.py:468(maybe_promote)\n",
      "        1    0.000    0.000    0.000    0.000 cast.py:674(infer_dtype_from)\n",
      "        1    0.000    0.000    0.000    0.000 cast.py:798(infer_dtype_from_array)\n",
      "        1    0.000    0.000    0.000    0.000 codecs.py:260(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 codecs.py:309(__init__)\n",
      "    49772    0.041    0.000    0.090    0.000 codecs.py:319(decode)\n",
      "        2    0.000    0.000    0.000    0.000 common.py:105(is_bool_indexer)\n",
      "        8    0.000    0.000    0.000    0.000 common.py:1148(needs_i8_conversion)\n",
      "       22    0.000    0.000    0.000    0.000 common.py:1240(is_float_dtype)\n",
      "       11    0.000    0.000    0.000    0.000 common.py:1274(is_bool_dtype)\n",
      "    15295    0.003    0.000    0.005    0.000 common.py:1416(is_1d_only_ea_dtype)\n",
      "       11    0.000    0.000    0.000    0.000 common.py:1429(is_extension_array_dtype)\n",
      "       96    0.000    0.000    0.000    0.000 common.py:145(classes)\n",
      "       96    0.000    0.000    0.000    0.000 common.py:147(<lambda>)\n",
      "       32    0.000    0.000    0.000    0.000 common.py:1483(is_ea_or_datetimelike_dtype)\n",
      "       25    0.000    0.000    0.000    0.000 common.py:150(classes_and_not_datetimelike)\n",
      "       25    0.000    0.000    0.000    0.000 common.py:155(<lambda>)\n",
      "       35    0.000    0.000    0.000    0.000 common.py:1552(get_dtype)\n",
      "      121    0.000    0.000    0.000    0.000 common.py:1587(_is_dtype_type)\n",
      "    30491    0.009    0.000    0.013    0.000 common.py:160(cast_scalar_indexer)\n",
      "       30    0.000    0.000    0.000    0.000 common.py:161(is_object_dtype)\n",
      "    15245    0.009    0.000    0.030    0.000 common.py:1721(validate_all_hashable)\n",
      "    30490    0.007    0.000    0.014    0.000 common.py:1740(<genexpr>)\n",
      "       14    0.000    0.000    0.000    0.000 common.py:1747(pandas_dtype)\n",
      "        3    0.000    0.000    0.000    0.000 common.py:208(any_not_none)\n",
      "        3    0.000    0.000    0.000    0.000 common.py:212(<genexpr>)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:222(count_not_none)\n",
      "        3    0.000    0.000    0.000    0.000 common.py:226(<genexpr>)\n",
      "       17    0.004    0.000    0.010    0.001 common.py:229(asarray_tuplesafe)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:254(index_labels_to_array)\n",
      "        3    0.000    0.000    0.000    0.000 common.py:281(maybe_make_list)\n",
      "        2    0.000    0.000    0.000    0.000 common.py:287(maybe_iterable_to_list)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:315(is_datetime64_dtype)\n",
      "    30484    0.007    0.000    0.010    0.000 common.py:346(apply_if_callable)\n",
      "        3    0.000    0.000    0.000    0.000 common.py:348(is_datetime64tz_dtype)\n",
      "       45    0.000    0.000    0.000    0.000 common.py:389(is_timedelta64_dtype)\n",
      "        5    0.000    0.000    0.000    0.000 common.py:459(is_interval_dtype)\n",
      "        9    0.000    0.000    0.000    0.000 common.py:497(is_categorical_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:533(is_string_or_object_np_dtype)\n",
      "    15286    0.006    0.000    0.017    0.000 common.py:552(require_length_match)\n",
      "       12    0.000    0.000    0.000    0.000 common.py:581(is_dtype_equal)\n",
      "        7    0.000    0.000    0.000    0.000 common.py:680(is_integer_dtype)\n",
      "        9    0.000    0.000    0.000    0.000 common.py:732(is_signed_integer_dtype)\n",
      "        2    0.000    0.000    0.000    0.000 common.py:75(get_op_result_name)\n",
      "        9    0.000    0.000    0.000    0.000 common.py:786(is_unsigned_integer_dtype)\n",
      "        2    0.000    0.000    0.000    0.000 common.py:97(_maybe_match_name)\n",
      "        1    0.000    0.000    0.000    0.000 concat.py:106(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 concat.py:111(<setcomp>)\n",
      "        2    0.000    0.000    0.000    0.000 concat.py:112(<genexpr>)\n",
      "        1    0.000    0.000    0.000    0.000 concat.py:117(<setcomp>)\n",
      "        2    0.000    0.000    0.000    0.000 concat.py:118(<genexpr>)\n",
      "        1    0.001    0.001    0.006    0.006 concat.py:185(concatenate_managers)\n",
      "        1    0.000    0.000    0.000    0.000 concat.py:208(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 concat.py:255(_maybe_reindex_columns_na_proxy)\n",
      "        2    0.000    0.000    0.000    0.000 concat.py:287(_get_mgr_concatenation_plan)\n",
      "        1    0.000    0.000    0.000    0.000 concat.py:363(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 concat.py:394(_is_valid_na_for)\n",
      "        1    0.000    0.000    0.000    0.000 concat.py:424(is_na)\n",
      "        1    0.000    0.000    0.003    0.003 concat.py:453(get_reindexed_values)\n",
      "        1    0.000    0.000    0.005    0.005 concat.py:535(_concatenate_join_units)\n",
      "        2    0.000    0.000    0.000    0.000 concat.py:547(<genexpr>)\n",
      "        1    0.000    0.000    0.003    0.003 concat.py:550(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 concat.py:588(_dtype_to_na_value)\n",
      "        1    0.000    0.000    0.000    0.000 concat.py:611(_get_empty_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 concat.py:641(_is_uniform_join_units)\n",
      "        2    0.000    0.000    0.000    0.000 concat.py:653(<genexpr>)\n",
      "        2    0.000    0.000    0.000    0.000 concat.py:656(<genexpr>)\n",
      "        2    0.000    0.000    0.000    0.000 concat.py:666(<genexpr>)\n",
      "        2    0.000    0.000    0.000    0.000 concat.py:669(<genexpr>)\n",
      "        2    0.000    0.000    0.000    0.000 concat.py:711(_combine_concat_plans)\n",
      "        1    0.000    0.000    0.000    0.000 concat.py:74(concat_compat)\n",
      "        2    0.000    0.000    0.000    0.000 concat.py:95(is_nonempty)\n",
      "    15248    0.020    0.000    0.048    0.000 config.py:109(_get_single_key)\n",
      "    15248    0.012    0.000    0.077    0.000 config.py:127(_get_option)\n",
      "    15248    0.007    0.000    0.084    0.000 config.py:255(__call__)\n",
      "    15248    0.004    0.000    0.004    0.000 config.py:571(_select_options)\n",
      "    15248    0.013    0.000    0.017    0.000 config.py:589(_get_root)\n",
      "    30496    0.012    0.000    0.012    0.000 config.py:603(_get_deprecated_option)\n",
      "    15248    0.004    0.000    0.009    0.000 config.py:630(_translate_key)\n",
      "    15248    0.005    0.000    0.013    0.000 config.py:642(_warn_if_deprecated)\n",
      "        5    0.040    0.008    0.316    0.063 construction.py:102(arrays_to_mgr)\n",
      "        3    0.000    0.000    0.295    0.098 construction.py:1051(_convert_object_array)\n",
      "       44    0.000    0.000    0.295    0.007 construction.py:1067(convert)\n",
      "        3    0.000    0.000    0.295    0.098 construction.py:1073(<listcomp>)\n",
      "        6    0.000    0.000    0.000    0.000 construction.py:233(mgr_to_mgr)\n",
      "        1    0.000    0.000    0.000    0.000 construction.py:274(ndarray_to_mgr)\n",
      "    15295    0.007    0.000    0.036    0.000 construction.py:379(extract_array)\n",
      "        1    0.000    0.000    0.000    0.000 construction.py:407(_check_values_indices_shape_match)\n",
      "        2    0.000    0.000    0.004    0.002 construction.py:425(dict_to_mgr)\n",
      "    15269    0.007    0.000    0.008    0.000 construction.py:438(ensure_wrapped_if_datetimelike)\n",
      "    15293    0.051    0.000    0.233    0.000 construction.py:470(sanitize_array)\n",
      "        2    0.000    0.000    0.000    0.000 construction.py:483(<listcomp>)\n",
      "        2    0.000    0.000    0.000    0.000 construction.py:486(<listcomp>)\n",
      "        2    0.000    0.000    0.000    0.000 construction.py:487(<listcomp>)\n",
      "        2    0.000    0.000    0.000    0.000 construction.py:494(<listcomp>)\n",
      "        3    0.000    0.000    1.195    0.398 construction.py:505(nested_data_to_arrays)\n",
      "        3    0.000    0.000    0.000    0.000 construction.py:534(treat_as_nested)\n",
      "        1    0.000    0.000    0.000    0.000 construction.py:549(_prep_ndarray)\n",
      "        5    0.000    0.000    0.036    0.007 construction.py:596(_homogenize)\n",
      "    15293    0.009    0.000    0.018    0.000 construction.py:630(_sanitize_ndim)\n",
      "        2    0.000    0.000    0.000    0.000 construction.py:635(_extract_index)\n",
      "    15293    0.005    0.000    0.006    0.000 construction.py:667(_sanitize_str_dtypes)\n",
      "    15293    0.005    0.000    0.006    0.000 construction.py:687(_maybe_repeat)\n",
      "    15293    0.019    0.000    0.112    0.000 construction.py:698(_try_cast)\n",
      "        1    0.000    0.000    0.000    0.000 construction.py:747(_get_axes)\n",
      "        3    0.012    0.004    1.195    0.398 construction.py:798(to_arrays)\n",
      "    15242    0.008    0.000    0.012    0.000 construction.py:805(is_empty_data)\n",
      "        1    0.000    0.000    0.002    0.002 construction.py:887(_list_to_arrays)\n",
      "        2    0.000    0.000    0.883    0.442 construction.py:934(_list_of_dict_to_arrays)\n",
      "   220812    0.163    0.000    0.175    0.000 construction.py:958(<genexpr>)\n",
      "        4    0.000    0.000    0.000    0.000 construction.py:959(<genexpr>)\n",
      "        2    0.036    0.018    0.036    0.018 construction.py:965(<listcomp>)\n",
      "        3    0.002    0.001    0.297    0.099 construction.py:971(_finalize_columns_and_data)\n",
      "        3    0.000    0.000    0.000    0.000 construction.py:993(_validate_or_indexify_columns)\n",
      "        3    0.000    0.000    0.000    0.000 dataclasses.py:1211(is_dataclass)\n",
      "   117407    0.596    0.000   10.233    0.000 decoder.py:332(decode)\n",
      "   117407    9.476    0.000    9.476    0.000 decoder.py:343(raw_decode)\n",
      "        2    0.000    0.000    0.000    0.000 dtype.py:216(construct_from_string)\n",
      "        2    0.000    0.000    0.000    0.000 dtypes.py:1144(construct_from_string)\n",
      "        5    0.000    0.000    0.000    0.000 dtypes.py:1206(is_dtype)\n",
      "        2    0.000    0.000    0.000    0.000 dtypes.py:300(construct_from_string)\n",
      "        2    0.000    0.000    0.000    0.000 dtypes.py:732(construct_from_string)\n",
      "        2    0.000    0.000    0.000    0.000 dtypes.py:891(construct_from_string)\n",
      "        7    0.000    0.000    0.000    0.000 enum.py:801(value)\n",
      "    15262    0.007    0.000    0.007    0.000 flags.py:47(__init__)\n",
      "       14    0.000    0.000    0.000    0.000 flags.py:51(allows_duplicate_labels)\n",
      "       14    0.000    0.000    0.000    0.000 flags.py:83(allows_duplicate_labels)\n",
      "        1    0.000    0.000    0.000    0.000 frame.py:10314(_get_agg_axis)\n",
      "        1    0.000    0.000    0.001    0.001 frame.py:10817(values)\n",
      "    15240    0.018    0.000    0.726    0.000 frame.py:1279(iterrows)\n",
      "        3    0.000    0.000    0.000    0.000 frame.py:1413(__len__)\n",
      "        1    0.000    0.000    0.001    0.001 frame.py:3273(transpose)\n",
      "        1    0.000    0.000    0.001    0.001 frame.py:3404(T)\n",
      "        5    0.000    0.000    0.002    0.000 frame.py:3411(_ixs)\n",
      "        4    0.000    0.000    0.003    0.001 frame.py:3463(__getitem__)\n",
      "        4    0.000    0.000    0.000    0.000 frame.py:3906(_box_col_values)\n",
      "        5    0.000    0.000    0.000    0.000 frame.py:3920(_clear_item_cache)\n",
      "        4    0.000    0.000    0.002    0.001 frame.py:3923(_get_item_cache)\n",
      "        1    0.000    0.000    0.055    0.055 frame.py:4615(_reindex_axes)\n",
      "        1    0.000    0.000    0.055    0.055 frame.py:4652(_reindex_columns)\n",
      "        2    0.000    0.000    0.056    0.028 frame.py:4788(reindex)\n",
      "        1    0.000    0.000    0.001    0.001 frame.py:4809(drop)\n",
      "        2    0.000    0.000    0.003    0.001 frame.py:5641(reset_index)\n",
      "       10    0.000    0.000    0.000    0.000 frame.py:578(_constructor)\n",
      "        1    0.000    0.000    0.013    0.013 frame.py:5859(isna)\n",
      "       15    0.001    0.000    1.515    0.101 frame.py:587(__init__)\n",
      "        1    0.000    0.000    0.022    0.022 frame.py:5882(dropna)\n",
      "        2    0.000    0.000    0.000    0.000 frame.py:804(axes)\n",
      "        1    0.000    0.000    0.000    0.000 frame.py:821(shape)\n",
      "        1    0.003    0.003    0.043    0.043 frame.py:8236(explode)\n",
      "        1    0.000    0.000    0.000    0.000 frame.py:881(_can_fast_transpose)\n",
      "        1    0.000    0.000    0.000    0.000 frame.py:898(_values)\n",
      "        1    0.002    0.002    0.021    0.021 frame.py:9109(join)\n",
      "        1    0.000    0.000    0.020    0.020 frame.py:9267(_join_compat)\n",
      "        1    0.000    0.000    0.003    0.003 frame.py:9940(_reduce)\n",
      "        1    0.000    0.000    0.000    0.000 frame.py:9987(blk_func)\n",
      "        3    0.000    0.000    0.000    0.000 fromnumeric.py:1008(_argsort_dispatcher)\n",
      "        3    0.000    0.000    0.001    0.000 fromnumeric.py:1012(argsort)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2401(_all_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2406(all)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2922(_prod_dispatcher)\n",
      "        1    0.000    0.000    0.001    0.001 fromnumeric.py:2927(prod)\n",
      "        3    0.000    0.000    0.001    0.000 fromnumeric.py:51(_wrapfunc)\n",
      "        2    0.000    0.000    0.001    0.001 fromnumeric.py:69(_wrapreduction)\n",
      "        2    0.000    0.000    0.000    0.000 fromnumeric.py:70(<dictcomp>)\n",
      "        5    0.000    0.000    0.000    0.000 function.py:49(__call__)\n",
      "        1    0.000    0.000    0.000    0.000 function_base.py:4995(_delete_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 function_base.py:4999(delete)\n",
      "        2    0.000    0.000    0.003    0.002 generic.py:10400(_logical_func)\n",
      "        2    0.000    0.000    0.003    0.002 generic.py:10470(all)\n",
      "        2    0.000    0.000    0.003    0.002 generic.py:10895(all)\n",
      "        1    0.001    0.001    0.002    0.002 generic.py:1516(__invert__)\n",
      "    15262    0.028    0.000    0.035    0.000 generic.py:239(__init__)\n",
      "       14    0.000    0.000    0.000    0.000 generic.py:328(attrs)\n",
      "       28    0.000    0.000    0.000    0.000 generic.py:349(flags)\n",
      "        1    0.000    0.000    0.002    0.002 generic.py:3609(take)\n",
      "        1    0.000    0.000    0.003    0.003 generic.py:3708(_take_with_is_copy)\n",
      "        2    0.000    0.000    0.000    0.000 generic.py:3925(_set_is_copy)\n",
      "        2    0.000    0.000    0.000    0.000 generic.py:4060(_check_inplace_and_allows_duplicate_labels)\n",
      "        1    0.000    0.000    0.001    0.001 generic.py:4238(drop)\n",
      "        1    0.000    0.000    0.001    0.001 generic.py:4274(_drop_axis)\n",
      "    46101    0.011    0.000    0.016    0.000 generic.py:43(_check)\n",
      "        2    0.000    0.000    0.000    0.000 generic.py:441(_validate_dtype)\n",
      "        2    0.000    0.000    0.056    0.028 generic.py:4719(reindex)\n",
      "        4    0.000    0.000    0.000    0.000 generic.py:4952(<genexpr>)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:4997(_needs_reindex_multi)\n",
      "        1    0.000    0.000    0.046    0.046 generic.py:5009(_reindex_with_indexers)\n",
      "        2    0.000    0.000    0.000    0.000 generic.py:514(_construct_axes_from_arguments)\n",
      "        2    0.000    0.000    0.000    0.000 generic.py:543(<dictcomp>)\n",
      "       18    0.000    0.000    0.000    0.000 generic.py:546(_get_axis_number)\n",
      "       16    0.001    0.000    0.001    0.000 generic.py:5517(__finalize__)\n",
      "        4    0.000    0.000    0.000    0.000 generic.py:554(_get_axis_name)\n",
      "    15245    0.011    0.000    0.011    0.000 generic.py:5561(__getattr__)\n",
      "    15261    0.026    0.000    0.099    0.000 generic.py:5577(__setattr__)\n",
      "        8    0.000    0.000    0.000    0.000 generic.py:560(_get_axis)\n",
      "        5    0.000    0.000    0.000    0.000 generic.py:5632(_protect_consolidate)\n",
      "        5    0.000    0.000    0.000    0.000 generic.py:5646(_consolidate_inplace)\n",
      "        5    0.000    0.000    0.000    0.000 generic.py:5650(f)\n",
      "        2    0.000    0.000    0.000    0.000 generic.py:566(_get_block_manager_axis)\n",
      "        1    0.000    0.000    0.001    0.001 generic.py:5718(dtypes)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:5749(astype)\n",
      "        2    0.000    0.000    0.003    0.001 generic.py:5926(copy)\n",
      "        3    0.000    0.000    0.000    0.000 generic.py:636(_info_axis)\n",
      "        5    0.000    0.000    0.000    0.000 generic.py:660(ndim)\n",
      "        1    0.000    0.000    0.001    0.001 generic.py:683(size)\n",
      "        3    0.000    0.000    0.000    0.000 generic.py:767(_set_axis)\n",
      "        1    0.000    0.000    0.003    0.003 indexing.py:1169(_getitem_axis)\n",
      "        1    0.001    0.001    0.001    0.001 indexing.py:130(iloc)\n",
      "        1    0.000    0.000    0.000    0.000 indexing.py:1437(_validate_integer)\n",
      "        1    0.000    0.000    0.000    0.000 indexing.py:1490(_getitem_axis)\n",
      "        1    0.000    0.000    0.000    0.000 indexing.py:2357(check_bool_indexer)\n",
      "    30484    0.017    0.000    0.027    0.000 indexing.py:2486(check_deprecated_indexers)\n",
      "        1    0.000    0.000    0.000    0.000 indexing.py:267(loc)\n",
      "        1    0.000    0.000    0.000    0.000 indexing.py:629(__call__)\n",
      "        2    0.000    0.000    0.003    0.001 indexing.py:954(__getitem__)\n",
      "        1    0.000    0.000    0.003    0.003 indexing.py:981(_getbool_axis)\n",
      "        5    0.000    0.000    0.000    0.000 inference.py:184(is_array_like)\n",
      "        3    0.000    0.000    0.000    0.000 inference.py:262(is_dict_like)\n",
      "        9    0.000    0.000    0.000    0.000 inference.py:288(<genexpr>)\n",
      "        3    0.000    0.000    0.000    0.000 inference.py:294(is_named_tuple)\n",
      "    30510    0.008    0.000    0.012    0.000 inference.py:321(is_hashable)\n",
      "        3    0.001    0.000    0.001    0.000 inference.py:390(is_dataclass)\n",
      "        1    0.000    0.000    0.000    0.000 inference.py:426(is_inferred_bool_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:191(isclass)\n",
      "        1    0.000    0.000    0.002    0.002 interactiveshell.py:277(_modified_open)\n",
      "        4    0.000    0.000    0.000    0.000 managers.py:1026(iget)\n",
      "        1    0.000    0.000    0.001    0.001 managers.py:1376(reduce)\n",
      "        1    0.000    0.000    0.001    0.001 managers.py:1551(as_array)\n",
      "        9    0.000    0.000    0.000    0.000 managers.py:156(blknos)\n",
      "        1    0.000    0.000    0.001    0.001 managers.py:1611(_interleave)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:1625(<listcomp>)\n",
      "       13    0.000    0.000    0.000    0.000 managers.py:1675(is_consolidated)\n",
      "       10    0.000    0.000    0.000    0.000 managers.py:1683(_consolidate_check)\n",
      "        3    0.000    0.000    0.000    0.000 managers.py:1689(<listcomp>)\n",
      "        8    0.003    0.000    0.134    0.017 managers.py:1693(_consolidate_inplace)\n",
      "    15247    0.008    0.000    0.008    0.000 managers.py:1714(__init__)\n",
      "        8    0.000    0.000    0.000    0.000 managers.py:172(blklocs)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:1736(from_blocks)\n",
      "    15242    0.018    0.000    0.136    0.000 managers.py:1745(from_array)\n",
      "    15246    0.003    0.000    0.003    0.000 managers.py:1806(_block)\n",
      "       16    0.000    0.000    0.000    0.000 managers.py:1851(dtype)\n",
      "    30492    0.017    0.000    0.021    0.000 managers.py:1862(internal_values)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:1925(create_block_manager_from_blocks)\n",
      "        5    0.000    0.000    0.241    0.048 managers.py:1951(create_block_manager_from_column_arrays)\n",
      "       46    0.000    0.000    0.000    0.000 managers.py:2006(_grouping_func)\n",
      "        5    0.003    0.001    0.107    0.021 managers.py:2022(_form_blocks)\n",
      "       12    0.089    0.007    0.103    0.009 managers.py:2074(_stack_arrays)\n",
      "        2    0.000    0.000    0.130    0.065 managers.py:2088(_consolidate)\n",
      "       16    0.000    0.000    0.002    0.000 managers.py:2093(<lambda>)\n",
      "        4    0.084    0.021    0.128    0.032 managers.py:2105(_merge_blocks)\n",
      "        3    0.000    0.000    0.000    0.000 managers.py:2116(<listcomp>)\n",
      "        3    0.000    0.000    0.000    0.000 managers.py:212(set_axis)\n",
      "        3    0.000    0.000    0.000    0.000 managers.py:2125(<listcomp>)\n",
      "        2    0.000    0.000    0.000    0.000 managers.py:2151(_preprocess_slice_or_indexer)\n",
      "        5    0.000    0.000    0.000    0.000 managers.py:217(is_single_block)\n",
      "       10    0.000    0.000    0.000    0.000 managers.py:222(items)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:226(get_dtypes)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:227(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:230(arrays)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:239(<listcomp>)\n",
      "        5    0.000    0.000    0.016    0.003 managers.py:253(apply)\n",
      "        5    0.000    0.000    0.000    0.000 managers.py:282(<dictcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:418(astype)\n",
      "        2    0.000    0.000    0.002    0.001 managers.py:578(copy)\n",
      "        4    0.000    0.000    0.000    0.000 managers.py:596(copy_func)\n",
      "        2    0.000    0.000    0.000    0.000 managers.py:599(<listcomp>)\n",
      "        5    0.000    0.000    0.000    0.000 managers.py:618(consolidate)\n",
      "        3    0.000    0.000    0.048    0.016 managers.py:634(reindex_indexer)\n",
      "        1    0.000    0.000    0.002    0.002 managers.py:689(<listcomp>)\n",
      "        2    0.000    0.000    0.045    0.023 managers.py:710(_slice_take_blocks_ax0)\n",
      "        1    0.000    0.000    0.002    0.002 managers.py:872(take)\n",
      "       15    0.000    0.000    0.000    0.000 managers.py:916(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:957(_verify_integrity)\n",
      "        2    0.000    0.000    0.000    0.000 managers.py:959(<genexpr>)\n",
      "        8    0.000    0.000    0.000    0.000 managers.py:970(from_blocks)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:980(fast_xs)\n",
      "        1    0.000    0.000    0.000    0.000 merge.py:1041(_get_merge_keys)\n",
      "        1    0.000    0.000    0.000    0.000 merge.py:1159(_maybe_coerce_merge_keys)\n",
      "        1    0.000    0.000    0.000    0.000 merge.py:1324(_validate_specification)\n",
      "        3    0.000    0.000    0.000    0.000 merge.py:2272(_any)\n",
      "        2    0.000    0.000    0.000    0.000 merge.py:2276(_validate_operand)\n",
      "        1    0.000    0.000    0.000    0.000 merge.py:2290(_items_overlap_with_suffix)\n",
      "        1    0.000    0.000    0.000    0.000 merge.py:611(__init__)\n",
      "        1    0.000    0.000    0.017    0.017 merge.py:712(get_result)\n",
      "        1    0.000    0.000    0.000    0.000 merge.py:746(_maybe_drop_cross_column)\n",
      "        1    0.000    0.000    0.000    0.000 merge.py:796(_maybe_restore_index_levels)\n",
      "        1    0.000    0.000    0.000    0.000 merge.py:832(_maybe_add_join_keys)\n",
      "        1    0.000    0.000    0.017    0.017 merge.py:90(merge)\n",
      "        1    0.000    0.000    0.010    0.010 merge.py:945(_get_join_info)\n",
      "        5    0.000    0.000    0.000    0.000 missing.py:107(clean_fill_method)\n",
      "      2/1    0.000    0.000    0.013    0.013 missing.py:150(_isna)\n",
      "        1    0.000    0.000    0.013    0.013 missing.py:227(_isna_array)\n",
      "        1    0.000    0.000    0.013    0.013 missing.py:268(_isna_string_dtype)\n",
      "        1    0.000    0.000    0.016    0.016 missing.py:288(notna)\n",
      "        7    0.000    0.000    0.001    0.000 missing.py:391(array_equivalent)\n",
      "        4    0.000    0.000    0.001    0.000 missing.py:495(_array_equivalent_object)\n",
      "        4    0.000    0.000    0.000    0.000 missing.py:572(na_value_for_dtype)\n",
      "      2/1    0.000    0.000    0.013    0.013 missing.py:67(isna)\n",
      "        5    0.000    0.000    0.000    0.000 missing.py:911(clean_reindex_fill_method)\n",
      "       59    0.000    0.000    0.000    0.000 multiarray.py:1071(copyto)\n",
      "        7    0.000    0.000    0.000    0.000 multiarray.py:148(concatenate)\n",
      "        2    0.000    0.000    0.000    0.000 nanops.py:191(_get_fill_value)\n",
      "        2    0.000    0.000    0.000    0.000 nanops.py:213(_maybe_get_mask)\n",
      "        2    0.000    0.000    0.000    0.000 nanops.py:257(_get_values)\n",
      "        2    0.000    0.000    0.000    0.000 nanops.py:346(_na_ok_dtype)\n",
      "        2    0.000    0.000    0.000    0.000 nanops.py:534(nanall)\n",
      "        1    0.000    0.000    0.000    0.000 numeric.py:133(__new__)\n",
      "        1    0.000    0.000    0.000    0.000 numeric.py:139(_ensure_array)\n",
      "        1    0.000    0.000    0.000    0.000 numeric.py:188(_validate_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 numeric.py:199(_ensure_dtype)\n",
      "       59    0.000    0.000    0.003    0.000 numeric.py:289(full)\n",
      "        2    0.000    0.000    0.000    0.000 numeric.py:331(_is_all_dates)\n",
      "        6    0.000    0.000    0.000    0.000 numerictypes.py:282(issubclass_)\n",
      "        3    0.000    0.000    0.000    0.000 numerictypes.py:356(issubdtype)\n",
      "        2    0.000    0.000    0.000    0.000 numerictypes.py:573(_can_coerce_all)\n",
      "       19    0.000    0.000    0.000    0.000 numerictypes.py:582(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 numerictypes.py:597(find_common_type)\n",
      "        1    0.000    0.000    0.000    0.000 numerictypes.py:649(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 numerictypes.py:650(<listcomp>)\n",
      "        5    0.000    0.000    0.000    0.000 range.py:167(_simple_new)\n",
      "        2    0.000    0.000    0.000    0.000 range.py:183(_constructor)\n",
      "        2    0.000    0.000    0.000    0.000 range.py:189(_data)\n",
      "        2    0.000    0.000    0.000    0.000 range.py:241(start)\n",
      "        2    0.000    0.000    0.000    0.000 range.py:264(stop)\n",
      "        2    0.000    0.000    0.000    0.000 range.py:287(step)\n",
      "        9    0.000    0.000    0.000    0.000 range.py:347(dtype)\n",
      "        3    0.000    0.000    0.000    0.000 range.py:351(is_unique)\n",
      "        1    0.000    0.000    0.000    0.000 range.py:356(is_monotonic_increasing)\n",
      "        2    0.000    0.000    0.000    0.000 range.py:372(inferred_type)\n",
      "    15240    0.002    0.000    0.002    0.000 range.py:427(__iter__)\n",
      "        2    0.000    0.000    0.000    0.000 range.py:522(equals)\n",
      "       72    0.000    0.000    0.000    0.000 range.py:909(__len__)\n",
      "        7    0.000    0.000    0.000    0.000 re.py:249(compile)\n",
      "        7    0.000    0.000    0.000    0.000 re.py:288(_compile)\n",
      "    30478    0.029    0.000    0.155    0.000 series.py:1052(_get_value)\n",
      "        4    0.000    0.000    0.000    0.000 series.py:1238(_set_as_cached)\n",
      "    15247    0.090    0.000    0.706    0.000 series.py:323(__init__)\n",
      "        1    0.000    0.000    0.017    0.017 series.py:4043(explode)\n",
      "        1    0.000    0.000    0.000    0.000 series.py:4435(_reduce)\n",
      "        2    0.000    0.000    0.000    0.000 series.py:523(_constructor)\n",
      "    15242    0.008    0.000    0.009    0.000 series.py:542(_set_axis)\n",
      "       16    0.000    0.000    0.000    0.000 series.py:575(dtype)\n",
      "    15249    0.017    0.000    0.028    0.000 series.py:590(name)\n",
      "    15244    0.015    0.000    0.045    0.000 series.py:640(name)\n",
      "    30492    0.012    0.000    0.032    0.000 series.py:687(_values)\n",
      "        5    0.000    0.000    0.000    0.000 series.py:743(__len__)\n",
      "        1    0.000    0.000    0.000    0.000 series.py:825(__array__)\n",
      "    30478    0.051    0.000    0.255    0.000 series.py:943(__getitem__)\n",
      "        3    0.000    0.000    0.000    0.000 shape_base.py:207(_arrays_for_stack_dispatcher)\n",
      "        3    0.000    0.000    0.000    0.000 shape_base.py:218(_vhstack_dispatcher)\n",
      "        3    0.000    0.000    0.042    0.014 shape_base.py:222(vstack)\n",
      "        3    0.000    0.000    0.000    0.000 shape_base.py:77(_atleast_2d_dispatcher)\n",
      "        3    0.000    0.000    0.000    0.000 shape_base.py:81(atleast_2d)\n",
      "        2    0.000    0.000    0.000    0.000 string_.py:118(construct_from_string)\n",
      "        8    0.000    0.000    0.047    0.006 take.py:120(_take_nd_ndarray)\n",
      "        8    0.000    0.000    0.000    0.000 take.py:326(_get_take_nd_function)\n",
      "        8    0.000    0.000    0.001    0.000 take.py:554(_take_preprocess_indexer_and_fill_value)\n",
      "        8    0.000    0.000    0.047    0.006 take.py:57(take_nd)\n",
      "        7    0.000    0.000    0.000    0.000 types.py:176(__get__)\n",
      "    30624    0.003    0.000    0.003    0.000 typing.py:1736(cast)\n",
      "        1    0.000    0.000    0.000    0.000 utils.py:249(maybe_convert_indices)\n",
      "        1    0.000    0.000    0.000    0.000 utils.py:457(check_array_indexer)\n",
      "        1    0.000    0.000    0.000    0.000 utils.py:69(is_list_like_indexer)\n",
      "        7    0.000    0.000    0.001    0.000 warnings.py:130(filterwarnings)\n",
      "        7    0.000    0.000    0.000    0.000 warnings.py:181(_add_filter)\n",
      "        7    0.000    0.000    0.000    0.000 warnings.py:437(__init__)\n",
      "        7    0.001    0.000    0.001    0.000 warnings.py:458(__enter__)\n",
      "        7    0.000    0.000    0.000    0.000 warnings.py:477(__exit__)\n",
      "       25    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x00007FFFC0A46CD0}\n",
      "       21    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}\n",
      "    49772    0.049    0.000    0.049    0.000 {built-in method _codecs.utf_8_decode}\n",
      "        1    0.035    0.035    0.035    0.035 {built-in method _collections._count_elements}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _operator.invert}\n",
      "       21    0.000    0.000    0.000    0.000 {built-in method _warnings._filters_mutated}\n",
      "15261/15260    0.007    0.000    0.021    0.000 {built-in method builtins.all}\n",
      "       11    0.000    0.000    0.000    0.000 {built-in method builtins.any}\n",
      "    30489    0.003    0.000    0.003    0.000 {built-in method builtins.callable}\n",
      "        1    0.003    0.003   16.618   16.618 {built-in method builtins.exec}\n",
      "    61459    0.006    0.000    0.006    0.000 {built-in method builtins.getattr}\n",
      "    15311    0.002    0.000    0.002    0.000 {built-in method builtins.hasattr}\n",
      "    30514    0.004    0.000    0.004    0.000 {built-in method builtins.hash}\n",
      "  5217882    0.365    0.000    0.380    0.000 {built-in method builtins.isinstance}\n",
      "    30736    0.003    0.000    0.003    0.000 {built-in method builtins.issubclass}\n",
      "300909/270295    0.037    0.000    0.044    0.000 {built-in method builtins.len}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
      "        4    0.003    0.001    0.005    0.001 {built-in method builtins.sorted}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.sum}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method fromkeys}\n",
      "        1    0.002    0.002    0.002    0.002 {built-in method io.open}\n",
      "        4    0.001    0.000    0.001    0.000 {built-in method numpy.arange}\n",
      "    15328    0.004    0.000    0.004    0.000 {built-in method numpy.array}\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method numpy.asanyarray}\n",
      "   100/99    0.000    0.000    0.000    0.000 {built-in method numpy.asarray}\n",
      "    78/72    0.044    0.001    0.047    0.001 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
      "       88    0.023    0.000    0.023    0.000 {built-in method numpy.empty}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method numpy.geterrobj}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method numpy.seterrobj}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method numpy.zeros}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method pandas._libs.lib.is_bool_array}\n",
      "        2    0.001    0.001    0.001    0.001 {built-in method pandas._libs.lib.is_datetime_array}\n",
      "        1    0.001    0.001    0.001    0.001 {method '__exit__' of '_io._IOBase' objects}\n",
      "        7    0.001    0.000    0.001    0.000 {method '_rebuild_blknos_and_blklocs' of 'pandas._libs.internals.BlockManager' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'add' of 'pandas._libs.internals.BlockPlacement' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'all' of 'numpy.ndarray' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'any' of 'numpy.ndarray' objects}\n",
      "   117489    0.017    0.000    0.017    0.000 {method 'append' of 'list' objects}\n",
      "        3    0.001    0.000    0.001    0.000 {method 'argsort' of 'numpy.ndarray' objects}\n",
      "       10    0.000    0.000    0.000    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'clear' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'copy' of 'dict' objects}\n",
      "        8    0.004    0.001    0.004    0.001 {method 'copy' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "   234814    0.025    0.000    0.025    0.000 {method 'end' of 're.Match' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}\n",
      "        3    0.001    0.000    0.001    0.000 {method 'format' of 'str' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "        1    0.001    0.001    0.001    0.001 {method 'get_indexer' of 'pandas._libs.index.IndexEngine' objects}\n",
      "    30482    0.011    0.000    0.011    0.000 {method 'get_loc' of 'pandas._libs.index.IndexEngine' objects}\n",
      "        7    0.000    0.000    0.000    0.000 {method 'insert' of 'list' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'item' of 'numpy.ndarray' objects}\n",
      "   310220    0.032    0.000    0.032    0.000 {method 'items' of 'dict' objects}\n",
      "   338221    0.028    0.000    0.028    0.000 {method 'keys' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}\n",
      "   234816    0.122    0.000    0.122    0.000 {method 'match' of 're.Pattern' objects}\n",
      "        2    0.000    0.000    0.003    0.001 {method 'max' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'nonzero' of 'numpy.ndarray' objects}\n",
      "       22    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'pop' of 'list' objects}\n",
      "       16    0.000    0.000    0.000    0.000 {method 'ravel' of 'numpy.ndarray' objects}\n",
      "        9    0.003    0.000    0.003    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "        7    0.000    0.000    0.000    0.000 {method 'remove' of 'list' objects}\n",
      "        1    0.001    0.001    0.001    0.001 {method 'repeat' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'search' of 're.Pattern' objects}\n",
      "    15248    0.004    0.000    0.004    0.000 {method 'split' of 'str' objects}\n",
      "   117413    0.027    0.000    0.027    0.000 {method 'startswith' of 'str' objects}\n",
      "   117407    0.059    0.000    0.059    0.000 {method 'strip' of 'str' objects}\n",
      "        3    0.001    0.000    0.001    0.000 {method 'take' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'transpose' of 'numpy.ndarray' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}\n",
      "   117407    0.152    0.000    0.152    0.000 {method 'update' of 'set' objects}\n",
      "       12    0.000    0.000    0.000    0.000 {method 'upper' of 'str' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
      "    15322    0.002    0.000    0.002    0.000 {pandas._libs.algos.ensure_object}\n",
      "       19    0.001    0.000    0.001    0.000 {pandas._libs.algos.ensure_platform_int}\n",
      "        4    0.002    0.000    0.002    0.000 {pandas._libs.algos.take_1d_int64_int64}\n",
      "        1    0.001    0.001    0.001    0.001 {pandas._libs.algos.take_2d_axis0_int64_int64}\n",
      "        3    0.038    0.013    0.038    0.013 {pandas._libs.algos.take_2d_axis0_object_object}\n",
      "        3    0.000    0.000    0.000    0.000 {pandas._libs.internals.get_blkno_placements}\n",
      "        4    0.000    0.000    0.000    0.000 {pandas._libs.lib.array_equivalent_object}\n",
      "        2    0.515    0.258    0.515    0.258 {pandas._libs.lib.dicts_to_array}\n",
      "        2    0.000    0.000    0.000    0.000 {pandas._libs.lib.dtypes_all_equal}\n",
      "        2    0.134    0.067    0.310    0.155 {pandas._libs.lib.fast_unique_multiple_list_gen}\n",
      "    15312    0.085    0.000    0.085    0.000 {pandas._libs.lib.infer_datetimelike_array}\n",
      "      9/8    0.000    0.000    0.000    0.000 {pandas._libs.lib.infer_dtype}\n",
      "        3    0.001    0.000    0.001    0.000 {pandas._libs.lib.is_all_arraylike}\n",
      "       10    0.000    0.000    0.000    0.000 {pandas._libs.lib.is_bool}\n",
      "    30492    0.004    0.000    0.004    0.000 {pandas._libs.lib.is_float}\n",
      "    60967    0.007    0.000    0.007    0.000 {pandas._libs.lib.is_integer}\n",
      "       15    0.002    0.000    0.002    0.000 {pandas._libs.lib.is_iterator}\n",
      "    45845    0.007    0.000    0.007    0.000 {pandas._libs.lib.is_list_like}\n",
      "    30494    0.004    0.000    0.004    0.000 {pandas._libs.lib.is_scalar}\n",
      "        6    0.000    0.000    0.000    0.000 {pandas._libs.lib.item_from_zerodim}\n",
      "       59    0.258    0.004    0.262    0.004 {pandas._libs.lib.maybe_convert_objects}\n",
      "        1    0.002    0.002    0.002    0.002 {pandas._libs.lib.to_object_array_tuples}\n",
      "        1    0.013    0.013    0.013    0.013 {pandas._libs.missing.isnaobj2d}\n",
      "        1    0.016    0.016    0.016    0.016 {pandas._libs.reshape.explode}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cProfile.run('q3_run_inicial(file_path)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memoria total utilizada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113474.8203125"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(memory_usage((q3_run_inicial, (file_path,))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código no optimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgdsghg\n",
    "\n",
    "# Lista para almacenar los DataFrames individuales\n",
    "data = []\n",
    "lista_keys = []\n",
    "\n",
    "# Abrir el archivo y leer línea por línea\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Crear un diccionariodataframe con una sola columna \"json\" para cada línea\n",
    "        diccionario_df = pd.DataFrame({'json': [line.strip()]})\n",
    "        # Agregar el DataFrame a la lista\n",
    "        data.append(diccionario_df)\n",
    "        lista_keys.extend(list(diccionario_df['json'].apply(json.loads)[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar todos los encabezados únicos para luego formar un dataframe genetal\n",
    "lista_keys_unicas = set(lista_keys)\n",
    "#Crear data frame vacío para luego concatenar\n",
    "df_inicial = pd.DataFrame(columns=lista_keys_unicas)\n",
    "#Iterar para concatenar todos los df\n",
    "for df in data: \n",
    "    df= df.reindex(columns=lista_keys_unicas)\n",
    "    df_inicial= pd.concat([df_inicial,df])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EDA_SIMPADE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
